{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "rpYGw1etWG8s",
    "outputId": "ad14e6e2-e5e8-4715-f01d-710e2d177372",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import sparcl\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import pickle\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, Gaussian1DKernel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sA9GgKwWWMM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, x, y, target_transform = False):\n",
    "    self.labels = y\n",
    "    self.spectrum = x\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    spectra = self.spectrum[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    if self.target_transform != False:\n",
    "      label = self.target_transform(label)\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "    return torch.tensor(spectra), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZgxYDifiI-r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "  def __init__(self, weight = [1, 1, 1], punish_distance = False, punish_Lya = False):\n",
    "    super(customLoss, self).__init__()\n",
    "    self.weight = weight\n",
    "    self.punish_distance = punish_distance\n",
    "    self.punish_Lya = punish_Lya\n",
    "\n",
    "  def forward(self, output, target):\n",
    "    weight = self.weight\n",
    "    target = target.permute(1, 0, 2)\n",
    "\n",
    "    if self.punish_distance == True:\n",
    "      punish = (torch.abs(torch.argmax(output[0], dim = 1) - torch.argmax(target[0], dim = 1)) * torch.sum(target[0], dim = 1) + (output[0] > 0.1).sum(dim = 1) * (1 - torch.sum(target[0], dim = 1)))\n",
    "      punish = (punish / max(torch.sum(punish).item(), 1) * 64).unsqueeze(dim = 1).tile(1, 78)\n",
    "      BCE = nn.BCELoss(reduction = \"mean\", weight = punish)\n",
    "    elif self.punish_Lya != False:\n",
    "      punish = torch.where(torch.sum(target[0], dim = -1) > 0, self.punish_Lya, 1)\n",
    "      punish = ((punish / torch.sum(punish).item()) * 64).unsqueeze(dim = -1).tile(1, 78)\n",
    "      BCE = nn.BCELoss(reduction = \"mean\", weight = punish)\n",
    "    else:\n",
    "      BCE = nn.BCELoss(reduction = \"mean\")\n",
    "\n",
    "    MSE = nn.MSELoss(reduction = \"mean\")\n",
    "\n",
    "    loss_1 = BCE(output[0], target[0])\n",
    "    loss_2 = MSE(torch.sum(output[1] * target[0], dim = -1), torch.sum(target[1], dim = -1))\n",
    "    loss_3 = MSE(torch.sum(output[2] * target[0], dim = -1), torch.sum(target[2], dim = -1))\n",
    "\n",
    "    loss_total = weight[0] * loss_1 + weight[1] * loss_2 + weight[2] * loss_3\n",
    "\n",
    "    return loss_total / sum(weight), loss_1, loss_2, loss_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkfeAzs3BCdB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\tdef __init__(self, kernel_size1_1, kernel_size1_4, kernel_size3_1, kernel_size3_2, kernel_size4_1, kernel_size4_2, ratio, out_channel7, pooling, fc_width1, fc_width2, fc_width3, rescale_1, rescale_2, rescale_3):\n",
    "\t\tself.rescale_1 = rescale_1\n",
    "\t\tself.rescale_2 = rescale_2\n",
    "\t\tself.rescale_3 = rescale_3\n",
    "\n",
    "\t\tsuper(ConvNet, self).__init__()\n",
    "\t\tkernel_size1_1 = kernel_size1_1\n",
    "\t\tkernel_size1_2 = round(kernel_size1_1 * ratio)\n",
    "\t\tkernel_size1_3 = round(kernel_size1_1 * ratio ** 2)\n",
    "\t\tkernel_size1_4 = kernel_size1_4\n",
    "\t\tstride = 1\n",
    "\t\tpadding1_1 = kernel_size1_1 // 2\n",
    "\t\tpadding1_2 = kernel_size1_2 // 2\n",
    "\t\tpadding1_3 = kernel_size1_3 // 2\n",
    "\t\tpadding1_4 = kernel_size1_4 // 2\n",
    "\t\tpadding3_1 = kernel_size3_1 // 2\n",
    "\t\tpadding3_2 = kernel_size3_2 // 2\n",
    "\t\tpadding4_1 = kernel_size4_1 // 2\n",
    "\t\tpadding4_2 = kernel_size4_2 // 2\n",
    "\t\tdilation = 1\n",
    "\t\tfactor = 2\n",
    "\t\tout_channel = 10 * factor\n",
    "\t\tout_channel2 = 10 * factor\n",
    "\t\tout_channel3 = 20 * factor\n",
    "\t\tout_channel4 = 20 * factor\n",
    "\t\tout_channel5 = 40 * factor\n",
    "\t\tout_channel6 = 80 * factor\n",
    "\t\tout_channel7 = out_channel7\n",
    "\n",
    "\t\tself.layer1_1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, out_channel, kernel_size = kernel_size1_1, stride = stride, padding = padding1_1, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel, out_channel2, kernel_size = kernel_size1_2, stride = stride, padding = padding1_2, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_3 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel2, out_channel3, kernel_size = kernel_size1_3, stride = stride, padding = padding1_3, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool1d(kernel_size = pooling, stride = pooling - 1, padding = pooling // 2),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_4 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel3, out_channel4, kernel_size = kernel_size1_4, stride = stride, padding = padding1_4, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel4),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool1d(kernel_size = pooling, stride = pooling - 1, padding = pooling // 2),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_5 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel4, out_channel5, kernel_size = 1, stride = stride, padding = 0, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel5),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_6 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel5, out_channel6, kernel_size = 1, stride = stride, padding = 0, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel6),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_7 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel6, out_channel7, kernel_size = 1, stride = stride, padding = 0, dilation = dilation),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdimension = math.floor(math.floor(2436 / pooling) / pooling) * out_channel7\n",
    "\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\tnn.Linear(610 * out_channel7, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, 78),\n",
    "\t \t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer3_1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, out_channel, kernel_size = kernel_size3_1, stride = stride, padding = padding3_1, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer3_2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel, out_channel2, kernel_size = kernel_size3_2, stride = stride, padding = padding3_2, dilation = dilation),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer4_1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, out_channel, kernel_size = 3, stride = stride, padding = 1, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer4_2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel, out_channel2, kernel_size = 3, stride = stride, padding = 1, dilation = dilation),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer5 = nn.Sequential(\n",
    "\t\t\tnn.Linear(1900, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer6 = nn.Sequential(\n",
    "\t\t\tnn.Linear(900, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, 1),\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif self.rescale_1 == \"mean\":\n",
    "\t\t\tx_scale_1 = x - torch.mean(x, dim = -1, keepdim = True)\n",
    "\t\telif self.rescale_1 == \"zscore\":\n",
    "\t\t\tx_scale_1 = (x - torch.mean(x, dim = -1, keepdim = True)) / torch.std(x, dim = -1, keepdim = True)\n",
    "\t\telse:\n",
    "\t\t\tx_scale_1 = torch.clone(x)\n",
    "\n",
    "\t\tx1_1 = self.layer1_1(x_scale_1)\n",
    "\t\tx1_2 = self.layer1_2(x1_1)\n",
    "\t\tx1_3 = self.layer1_3(x1_2)\n",
    "\t\tx1_4 = self.layer1_4(x1_3)\n",
    "\t\tx1_5 = self.layer1_5(x1_4)\n",
    "\t\tx1_6 = self.layer1_6(x1_5)\n",
    "\t\tx1_7 = self.layer1_7(x1_6)\n",
    "\t\tx1_flatten = x1_7.view(x1_7.size(0), -1)\n",
    "\t\tx2 = self.layer2(x1_flatten)\n",
    "\t\tx2_mask = torch.zeros_like(x2).to(device)\n",
    "\t\tx2_mask[x2 > 0.7] = 1\n",
    "\n",
    "\t\tcrop_index1 = torch.round(torch.argmax(x2, dim = -1) * 31.25 + 15.625).unsqueeze(1) + torch.arange(-47, 48).to(device) + 48\n",
    "\t\tcrop_index1 = crop_index1.type(torch.long).to(device)\n",
    "\t\tx = x.squeeze(dim = 1)\n",
    "\t\tx_extended = torch.cat((x, torch.full_like(x[:, :48], torch.median(x)).to(device)), dim = -1)\n",
    "\t\tx_extended = torch.cat((torch.full_like(x[:, :48], torch.median(x)).to(device), x_extended), dim = 1)\n",
    "\t\tx_cropped = x_extended[torch.arange(x_extended.size(0)).unsqueeze(1).to(device), crop_index1]\n",
    "\t\tx_cropped = x_cropped.unsqueeze(dim = 1)\n",
    "\t\tx = x.unsqueeze(dim = 1)\n",
    "\n",
    "\t\tif self.rescale_2 == \"mean\":\n",
    "\t\t\tx_scale_2 = x_cropped - torch.mean(x_cropped, dim = -1, keepdim = True)\n",
    "\t\telif self.rescale_2 == \"zscore\":\n",
    "\t\t\tx_scale_2 = (x_cropped - torch.mean(x_cropped, dim = -1, keepdim = True)) / torch.std(x_cropped, dim = -1, keepdim = True)\n",
    "\t\telse:\n",
    "\t\t\tx_scale_2 = torch.clone(x_cropped)\n",
    "\n",
    "\t\tx3_1 = self.layer3_1(x_scale_2)\n",
    "\t\tx3_2 = self.layer3_2(x3_1)\n",
    "\t\tx3_flatten = x3_2.view(x3_2.size(0), -1)\n",
    "\n",
    "\t\tx5 = self.layer5(x3_flatten)\n",
    "\t\tcrop_index2 = torch.round(torch.sum(x5, dim = -1) * x_cropped.size(-1)).unsqueeze(1) + torch.arange(-22, 23).to(device) + 23\n",
    "\t\tx5 = x2_mask * x5\n",
    "\n",
    "\t\tcrop_index2 = crop_index2.type(torch.long).to(device)\n",
    "\t\tx_cropped = x_cropped.squeeze(dim = 1)\n",
    "\t\tx_cropped2 = torch.cat((torch.full_like(x_cropped[:, :23], torch.median(x_cropped)).to(device), x_cropped, torch.full_like(x_cropped[:, :23], torch.median(x_cropped)).to(device)), dim = -1)\n",
    "\t\tx_cropped2 = x_cropped2[torch.arange(x_cropped2.size(0)).unsqueeze(1).to(device), crop_index2]\n",
    "\t\tx_cropped2 = x_cropped2.unsqueeze(dim = 1)\n",
    "\t\tx_cropped = x_cropped.unsqueeze(dim = 1)\n",
    "\n",
    "\t\tif self.rescale_3 == \"mean\":\n",
    "\t\t\tx_scale_3 = x_cropped2 - torch.mean(x_cropped2, dim = -1, keepdim = True)\n",
    "\t\telif self.rescale_3 == \"zscore\":\n",
    "\t\t\tx_scale_3 = (x_cropped2 - torch.mean(x_cropped2, dim = -1, keepdim = True)) / torch.std(x_cropped2, dim = -1, keepdim = True)\n",
    "\t\telse:\n",
    "\t\t\tx_scale_3 = torch.clone(x_cropped2)\n",
    "\n",
    "\t\tx4_1 = self.layer4_1(x_scale_3)\n",
    "\t\tx4_2 = self.layer4_2(x4_1)\n",
    "\t\tx4_flatten = x4_2.view(x4_2.size(0), -1)\n",
    "\n",
    "\t\tx6 = self.layer6(x4_flatten)\n",
    "\t\tx6 = x2_mask * x6\n",
    "\n",
    "\t\tx2 = torch.unsqueeze(x2, dim = 0)\n",
    "\t\tx5 = torch.unsqueeze(x5, dim = 0)\n",
    "\t\tx6 = torch.unsqueeze(x6, dim = 0)\n",
    "\n",
    "\t\treturn torch.cat((x2, x5, x6), 0), x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thksTRyeMvIG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../DESI_LAE_dataset/LAE_spectra_b_band.pkl\", \"rb\") as fh:\n",
    "    LAE_spectra = pickle.load(fh)\n",
    "    LAE_flux = np.array([np.array(i[\"flux\"]).reshape(1, -1) for i in LAE_spectra])\n",
    "    LAE_ivar = np.array([np.array(i[\"ivar\"]).reshape(1, -1) ** (1 / 2) for i in LAE_spectra])\n",
    "    LAE = LAE_flux * LAE_ivar\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/ELG_spectra_b_band.pkl\", \"rb\") as fh:\n",
    "    ELG_spectra = pickle.load(fh)\n",
    "    ELG_flux = np.array([np.array(i[\"flux\"]).reshape(1, -1) for i in ELG_spectra])\n",
    "    ELG_ivar = np.array([np.array(i[\"ivar\"]).reshape(1, -1) ** (1 / 2) for i in ELG_spectra])\n",
    "    ELG = ELG_flux * ELG_ivar\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/QSO_spectra_b_band.pkl\", \"rb\") as fh:\n",
    "    QSO_spectra = pickle.load(fh)\n",
    "    QSO_flux = np.array([np.array(i[\"flux\"]).reshape(1, -1) for i in QSO_spectra])\n",
    "    QSO_ivar = np.array([np.array(i[\"ivar\"]).reshape(1, -1) ** (1 / 2) for i in QSO_spectra])\n",
    "    QSO = QSO_flux * QSO_ivar\n",
    "\n",
    "LAE_flux_train, LAE_flux_test = train_test_split(LAE, test_size = 0.2, random_state = 2)\n",
    "ELG_flux_train, ELG_flux_test = train_test_split(ELG, test_size = 0.2, random_state = 2)\n",
    "QSO_flux_train, QSO_flux_test = train_test_split(QSO, test_size = 0.2, random_state = 2)\n",
    "\n",
    "x_train = np.concatenate((LAE_flux_train, ELG_flux_train, QSO_flux_train), axis = 0)\n",
    "x_test = np.concatenate((LAE_flux_test, ELG_flux_test, QSO_flux_test), axis = 0)\n",
    "\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/LAE_25.pkl\", \"rb\") as fh:\n",
    "    LAE_label = pickle.load(fh)\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/others_25.pkl\", \"rb\") as fh:\n",
    "    other_label = pickle.load(fh)\n",
    "\n",
    "LAE_label_train, LAE_label_test = train_test_split(LAE_label, test_size = 0.2, random_state = 2)\n",
    "other_label_train, other_label_test = train_test_split(other_label, test_size = 0.2, random_state = 2)\n",
    "\n",
    "y_train = np.concatenate((LAE_label_train, other_label_train), axis = 0)\n",
    "y_test = np.concatenate((LAE_label_test, other_label_test), axis = 0)\n",
    "\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(y_train)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kgP3VX3BW1kj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, criterion, train_loader):\n",
    "    train_loss_iter = 0\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    correct_3 = 0\n",
    "    output1_record = torch.tensor([[], []])\n",
    "    output2_record = torch.tensor([[], []])\n",
    "    output3_record = torch.tensor([[], []])\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model = model.to(device).float()\n",
    "        output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data.float())\n",
    "\n",
    "        loss, loss_1, loss_2, loss_3 = criterion(output.float(), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_iter += loss\n",
    "\n",
    "        output = output.detach()\n",
    "\n",
    "        output1 = output[0].argmax(dim = -1)\n",
    "        output1[output[0].max(dim = -1)[0] < 0.7] = 0\n",
    "        target1 = target.permute(1, 0, 2)[0].argmax(dim = -1)\n",
    "        difference1 = (output1 - target1) ** 2\n",
    "        correct_1 += len(difference1[difference1 < 1])\n",
    "        output1_record = torch.cat((output1_record.to(\"cpu\"), torch.cat((output1.unsqueeze(0).to(\"cpu\"), target1.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "        mask1 = torch.where(output[0] > 0.7, 1, 0)\n",
    "        # mask2 = torch.where(difference_1 > 0, -1, 1)\n",
    "\n",
    "        output2 = torch.sum(output[1] * mask1, dim = -1)\n",
    "        target2 = torch.sum((target.permute(1, 0, 2)[1]), dim = -1)\n",
    "        difference2 = (output2 - target2) ** 2\n",
    "        # difference_2 = difference_2 * mask2\n",
    "        # difference_2 = difference_2\n",
    "        # difference_2 = difference_2[difference_2 >= 0]\n",
    "        correct_2 += len(difference2[difference2 < 0.0025])\n",
    "        output2_record = torch.cat((output2_record.to(\"cpu\"), torch.cat((output2.unsqueeze(0).to(\"cpu\"), target2.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "        output3 = torch.sum(output[2] * mask1, dim = -1)\n",
    "        target3 = torch.sum((target.permute(1, 0, 2)[2]), dim = -1)\n",
    "        difference3 = (output3 - target3) ** 2\n",
    "        # difference_3 = difference_3 * mask2\n",
    "        # difference_3 = difference_3\n",
    "        # difference_3 = difference_3[difference_3 >= 0]\n",
    "        correct_3 += len(difference3[difference3 < 9])\n",
    "        output3_record = torch.cat((output3_record.to(\"cpu\"), torch.cat((output3.unsqueeze(0).to(\"cpu\"), target3.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "    train_loss_iter /= len(train_loader.dataset)\n",
    "\n",
    "    return model, train_loss_iter, output1_record, output2_record, output3_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lSJxXfdYfhjl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(epoch, model, criterion, test_loader, ray_tune):\n",
    "    test_loss_iter = 0\n",
    "    test_loss1_iter = 0\n",
    "    test_loss2_iter = 0\n",
    "    test_loss3_iter = 0\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    correct_3 = 0\n",
    "    output1_record = torch.tensor([[], []])\n",
    "    output2_record = torch.tensor([[], []])\n",
    "    output3_record = torch.tensor([[], []])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            model = model.to(device).float()\n",
    "            output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data.float())\n",
    "\n",
    "            loss, loss_1, loss_2, loss_3 = criterion(output.float(), target.float())\n",
    "            test_loss_iter += loss\n",
    "            test_loss1_iter += loss_1\n",
    "            test_loss2_iter += loss_2\n",
    "            test_loss3_iter += loss_3\n",
    "\n",
    "            output1 = output[0].argmax(dim = -1)\n",
    "            output1[output[0].max(dim = -1)[0] < 0.7] = 0\n",
    "            target1 = target.permute(1, 0, 2)[0].argmax(dim = -1)\n",
    "            difference1 = (output1 - target1) ** 2\n",
    "            correct_1 += len(difference1[difference1 < 1])\n",
    "            output1_record = torch.cat((output1_record.to(\"cpu\"), torch.cat((output1.unsqueeze(0).to(\"cpu\"), target1.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "\n",
    "            mask1 = torch.where(output[0] > 0.7, 1, 0)\n",
    "            # mask2 = torch.where(difference_1 > 0, -1, 1)\n",
    "\n",
    "            output2 = torch.sum(output[1] * mask1, dim = -1)\n",
    "            target2 = torch.sum((target.permute(1, 0, 2)[1]), dim = -1)\n",
    "            difference2 = (output2 - target2) ** 2\n",
    "            # difference_2 = difference_2 * mask2\n",
    "            # difference_2 = difference_2\n",
    "            # difference_2 = difference_2[difference_2 >= 0]\n",
    "            correct_2 += len(difference2[difference2 < 0.0025])\n",
    "            output2_record = torch.cat((output2_record.to(\"cpu\"), torch.cat((output2.unsqueeze(0).to(\"cpu\"), target2.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "            output3 = torch.sum(output[2] * mask1, dim = -1)\n",
    "            target3 = torch.sum((target.permute(1, 0, 2)[2]), dim = -1)\n",
    "            difference3 = (output3 - target3) ** 2\n",
    "            # difference_3 = difference_3 * mask2\n",
    "            # difference_3 = difference_3\n",
    "            # difference_3 = difference_3[difference_3 >= 0]\n",
    "            correct_3 += len(difference3[difference3 < 9])\n",
    "            output3_record = torch.cat((output3_record.to(\"cpu\"), torch.cat((output3.unsqueeze(0).to(\"cpu\"), target3.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "    data_count = len(test_loader.dataset)\n",
    "\n",
    "    test_loss_iter /= data_count\n",
    "    test_loss1_iter /= data_count\n",
    "    test_loss2_iter /= data_count\n",
    "    test_loss3_iter /= data_count\n",
    "\n",
    "    if not ray_tune:\n",
    "        print(f'平均損失: {test_loss_iter:.6f}, 1st loss: {test_loss1_iter:.6f}({correct_1 / data_count * 100:.1f}), 2nd loss: {test_loss2_iter:.6f}({correct_2 / data_count * 100:.1f}), 3rd loss: {test_loss3_iter:.6f}({correct_3 / data_count * 100:.1f})')\n",
    "\n",
    "    return model, test_loss_iter, [correct_1 / data_count, correct_2 / data_count, correct_3 / data_count], output1_record, output2_record, output3_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TqShYOTHJS8u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity = 'relu', mode = 'fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-48MBtWOXT66",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(config, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test, ray_tune = True):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_acc_1 = []\n",
    "    test_acc_2 = []\n",
    "    test_acc_3 = []\n",
    "    output1_record_train = []\n",
    "    output2_record_train = []\n",
    "    output3_record_train = []\n",
    "    output1_record_test = []\n",
    "    output2_record_test = []\n",
    "    output3_record_test = []\n",
    "\n",
    "    train_dataset = CustomDataset(x = x_train, y = y_train)\n",
    "    test_dataset = CustomDataset(x = x_test, y = y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config[\"batch_size\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size = config[\"batch_size\"])\n",
    "\n",
    "    model = ConvNet(kernel_size1_1 = config[\"kernel_size1_1\"], kernel_size4_2 = config[\"kernel_size4_2\"],\n",
    "                  kernel_size1_4 = config[\"kernel_size1_4\"], kernel_size3_1 = config[\"kernel_size3_1\"],\n",
    "                  kernel_size3_2 = config[\"kernel_size3_2\"], kernel_size4_1 = config[\"kernel_size4_1\"],\n",
    "                  ratio = config[\"ratio\"], out_channel7 = config[\"out_channel7\"],\n",
    "                  pooling = config[\"pooling\"], fc_width1 = config[\"fc_width1\"],\n",
    "                  fc_width2 = config[\"fc_width2\"], fc_width3 = config[\"fc_width3\"],\n",
    "                  rescale_1 = config[\"rescale_1\"], rescale_2 = config[\"rescale_2\"],\n",
    "                  rescale_3 = config[\"rescale_3\"])\n",
    "\n",
    "    criterion = customLoss(weight = [config[\"loss_weight_1\"], config[\"loss_weight_2\"], config[\"loss_weight_3\"]], punish_Lya = config[\"LAE_weight\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr = config[\"lr\"])\n",
    "\n",
    "    epoch = 35\n",
    "\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    for i in range(1, epoch + 1):\n",
    "        # optimizer = optim.SGD(model.parameters(), lr = (config[\"lr\"] * (1 - i / 60)) ** (1 / 2),\n",
    "        #             momentum = config[\"momentum\"], weight_decay = config[\"weight_decay\"])\n",
    "\n",
    "        model.train()\n",
    "        model, train_loss_iter, output1_record, output2_record, output3_record = train(epoch = i, model = model, criterion = criterion, optimizer = optimizer, train_loader = train_loader)\n",
    "        train_loss.append(train_loss_iter)\n",
    "        output1_record_train.append(output1_record)\n",
    "        output2_record_train.append(output2_record)\n",
    "        output3_record_train.append(output3_record)\n",
    "\n",
    "        model.eval()\n",
    "        model, test_loss_iter, test_accuracy_iter, output1_record, output2_record, output3_record = test(epoch = i, model = model, criterion = criterion, test_loader = test_loader, ray_tune = ray_tune)\n",
    "        test_loss.append(test_loss_iter)\n",
    "        test_acc_1.append(test_accuracy_iter[0])\n",
    "        test_acc_2.append(test_accuracy_iter[1])\n",
    "        test_acc_3.append(test_accuracy_iter[2])\n",
    "        output1_record_test.append(output1_record)\n",
    "        output2_record_test.append(output2_record)\n",
    "        output3_record_test.append(output3_record)\n",
    "\n",
    "        if ray_tune:\n",
    "            ray.train.report({\"mean_accuracy\": test_acc_1[test_loss.index(min(test_loss))]})\n",
    "        else:\n",
    "            if test_accuracy_iter[0] > 0.975:\n",
    "                current_time = datetime.datetime.now()\n",
    "                torch.save(model, f\"../DESI_LAE_model/model_{test_accuracy_iter[0]:.3f}_{test_accuracy_iter[1]:.3f}_{test_accuracy_iter[2]:.3f}_{current_time}.pt\")\n",
    "                print(\"Saved!\")\n",
    "                \n",
    "    if not ray_tune:\n",
    "        return model, train_loss, test_loss, test_acc_1, test_acc_2, test_acc_3, output1_record_train, output2_record_train, output3_record_train, output1_record_test, output2_record_test, output3_record_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_iter(test_loss, train_loss):\n",
    "    plt.close()\n",
    "    epoch = [i for i in range(1, len(test_loss) + 1)]\n",
    "    plt.plot(epoch, test_loss, label = \"Test Loss\", c = \"m\")\n",
    "    plt.plot(epoch, train_loss, label = \"Train Loss\", c = \"y\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    # plt.savefig(\"/content/drive/MyDrive/loss.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_accuracy(test_acc1, test_acc2, test_acc3):\n",
    "    plt.close()\n",
    "    epoch = [i for i in range(1, len(test_acc1) + 1)]\n",
    "    plt.plot(epoch, test_acc1, label = \"Rough Position\", c = \"r\")\n",
    "    plt.plot(epoch, test_acc2, label = \"Precise Position\", c = \"m\")\n",
    "    plt.plot(epoch, test_acc3, label = \"FWHM\", c = \"y\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    # plt.savefig(\"/content/drive/MyDrive/loss.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "3jJJoFl5cak5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_perform(model, spectra, label, info_spectra, plot_wrong = False):\n",
    "    with torch.no_grad():\n",
    "        data = torch.tensor(spectra).to(device).float()\n",
    "        model = model.to(device).float()\n",
    "        output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data)\n",
    "        output = output.to(\"cpu\")\n",
    "    \n",
    "    print(output.shape)\n",
    "    mask1 = torch.where(output[0] > 0.7, 1, 0)\n",
    "    a = torch.argmax(mask1, axis = -1)\n",
    "    b = torch.argmax(torch.tensor(label).permute(1, 0, 2)[0], axis = -1)\n",
    "    diff = torch.argwhere(a - b).squeeze(dim = -1)\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "\n",
    "    x_range = info_spectra[0][\"WAVE\"]\n",
    "    pixel_range = np.array([i * 25 + 3600 for i in range(0, 78)])\n",
    "    \n",
    "    plt.close()\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    plt.figure(dpi = 200)\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(x = a, y = b, c = \"m\", s = 5)\n",
    "    plt.xlabel(xlabel = \"Predicted Position\")\n",
    "    plt.ylabel(ylabel = \"True Position\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.scatter(x = torch.sum(mask1 * output[1], dim = -1), y = torch.sum(torch.tensor(label).permute(1, 0, 2)[1], axis = -1), c = \"m\", s = 5)\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100) - 0.05, c = \"k\")\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100) + 0.05, c = \"k\")\n",
    "    plt.xlabel(xlabel = \"Predicted Percentage\")\n",
    "    plt.ylabel(ylabel = \"True Percentage\")\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.scatter(x = torch.sum(mask1 * output[2], dim = -1), y = torch.sum(torch.tensor(label).permute(1, 0, 2)[2], axis = -1), c = \"m\", s = 5)\n",
    "    plt.plot(np.linspace(0, 40, 100), np.linspace(0, 40, 100) - 3, c = \"k\")\n",
    "    plt.plot(np.linspace(0, 40, 100), np.linspace(0, 40, 100) + 3, c = \"k\")\n",
    "    plt.xlabel(xlabel = \"Predicted Width (pixel)\")\n",
    "    plt.ylabel(ylabel = \"True Width (pixel)\")\n",
    "    \n",
    "    plt.savefig(f\"../DESI_LAE_diagram/scatter_{current_time}.jpg\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # for i in diff:\n",
    "    #     print(\"=======================================================================================================================\")\n",
    "    #     if (a[i].item() == 0) & (b[i].item() != 0):\n",
    "    #         print(\"Ignored LAE.\")\n",
    "    #     elif (a[i].item() != 0) & (b[i].item() == 0):\n",
    "    #         print(\"NLAE detected.\")\n",
    "    #     elif (a[i] - b[i]).item() ** 2 == 1:\n",
    "    #         print(\"Nearby offset.\")\n",
    "    #     elif (a[i] - b[i]).item() ** 2 > 1:\n",
    "    #         print(\"Offset.\")\n",
    "    #     print(f\"Prediction: {a[i].item()}\")\n",
    "    #     print(f\"True: {b[i].item()}\")\n",
    "    #     print(output[0][i])\n",
    "    #     print(torch.tensor(label).permute(1, 0, 2)[0][i])\n",
    "        \n",
    "    if plot_wrong:\n",
    "        plt.close()\n",
    "        plt.rcParams['figure.figsize'] = [16, 4]\n",
    "        plt.plot(x_range, x_scale_1[i][0].to(\"cpu\"), alpha = 1, c = \"r\", lw = 1)\n",
    "        plt.bar(pixel_range, output[0][i].to(\"cpu\") * torch.max(x_scale_1[i][0].to(\"cpu\")).item(), width = 25, align = \"edge\")\n",
    "        plt.title(str(info_spectra[i]['specid']))\n",
    "        print(str(info_spectra[i]['specid']))\n",
    "        # plt.savefig(\"/content/drive/MyDrive/CNN/result/x_scale_3/\" + str(info_spectra[i]['specid']) + \".jpg\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    cm = confusion_matrix(y_pred = torch.max(mask1, dim = -1)[0], y_true = torch.sum(torch.tensor(label).permute(1, 0, 2)[0], dim = -1))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"NLAE\", \"LAE\"])\n",
    "    disp.plot()\n",
    "    # plt.figure(dpi = 200)\n",
    "    plt.savefig(f\"../DESI_LAE_diagram/confusion_matrix_{current_time}.jpg\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "    return output, torch.tensor(label).permute(1, 0, 2), diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../DESI_LAE_dataset/train_spectra/LAE_iron_b.pkl\", \"rb\") as fh:\n",
    "    LAE_spectra = pickle.load(fh)\n",
    "    LAE_flux = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in LAE_spectra])\n",
    "    LAE_ivar = np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in LAE_spectra])\n",
    "    LAE = LAE_flux * LAE_ivar\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/train_spectra/NLAE_iron_b.pkl\", \"rb\") as fh:\n",
    "    NLAE_spectra = pickle.load(fh)\n",
    "    NLAE_flux = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in NLAE_spectra])\n",
    "    NLAE_ivar = np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in NLAE_spectra])\n",
    "    NLAE = NLAE_flux * NLAE_ivar\n",
    "    \n",
    "test_iron_spetra = np.concatenate((LAE, NLAE), axis = 0)\n",
    "info_spectra = LAE_spectra + NLAE_spectra\n",
    "    \n",
    "with open(\"../DESI_LAE_dataset/train_label/LAE_iron_25.pkl\", \"rb\") as fh:\n",
    "  LAE_label = pickle.load(fh)\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/train_label/NLAE_iron_25.pkl\", \"rb\") as fh:\n",
    "  NLAE_label = pickle.load(fh)\n",
    "\n",
    "test_iron_label = np.concatenate((LAE_label, NLAE_label), axis = 0)\n",
    "\n",
    "model = torch.load(f\"../DESI_LAE_model/model_0.977_0.974_0.891_2024-02-07 12:05:21.648665.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, label, diff = test_perform(model = model, spectra = test_iron_spetra, label = test_iron_label, info_spectra = info_spectra, plot_wrong = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mb79cd0YnNrr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_LAE(model):\n",
    "  with torch.no_grad():\n",
    "    data = torch.tensor(LAE_flux_test).to(device).float()\n",
    "    model = model.to(device).float()\n",
    "    output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data)\n",
    "  return x_cropped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimal_perform(search_space):\n",
    "    n = 0\n",
    "    while True:\n",
    "        n += 1\n",
    "        print(\"=======================================================================================================================\")\n",
    "        print(f\"No.{n} trial:\")\n",
    "        model, train_loss, test_loss, test_acc_1, test_acc_2, test_acc_3, output1_record_train, output2_record_train, output3_record_train, output1_record_test, output2_record_test, output3_record_test = build_model(config = search_space, ray_tune = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxjmJ44zHLVD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": 0.009,\n",
    "    \"kernel_size1_1\": 21,\n",
    "    \"kernel_size1_4\": 3,\n",
    "    \"kernel_size3_1\": 21,\n",
    "    \"kernel_size3_2\": 5,\n",
    "    \"kernel_size4_1\": 3,\n",
    "    \"kernel_size4_2\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"ratio\": 1.9,\n",
    "    \"out_channel7\": 100,\n",
    "    \"pooling\": 3,\n",
    "    \"loss_weight_1\": 100,\n",
    "    \"loss_weight_2\": 1,\n",
    "    \"loss_weight_3\": 10,\n",
    "    \"fc_width1\": 256,\n",
    "    \"fc_width2\": 256,\n",
    "    \"fc_width3\": 256,\n",
    "    \"rescale_1\": \"mean\",\n",
    "    \"rescale_2\": \"mean\",\n",
    "    \"rescale_3\": \"mean\",\n",
    "    \"LAE_weight\": 5\n",
    "    # \"momentum\": tune.grid_search([1.1]),\n",
    "    # \"weight_decay\": tune.grid_search([0])\n",
    "}\n",
    "\n",
    "optimal_perform(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zaNyGbs-JG2",
    "outputId": "5dbdd51d-fdd8-4b6a-fe1c-917eb2151600",
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": tune.grid_search([0.006]),\n",
    "    \"kernel_size1_1\": tune.grid_search([21]),\n",
    "    \"kernel_size1_4\": tune.grid_search([3]),\n",
    "    \"kernel_size3_1\": tune.grid_search([21]),\n",
    "    \"kernel_size3_2\": tune.grid_search([5]),\n",
    "    \"kernel_size4_1\": tune.grid_search([3]),\n",
    "    \"kernel_size4_2\": tune.grid_search([3]),\n",
    "    \"batch_size\": tune.grid_search([256]),\n",
    "    \"ratio\": tune.grid_search([1.9]),\n",
    "    \"out_channel7\": tune.grid_search([100]),\n",
    "    \"pooling\": tune.grid_search([3]),\n",
    "    \"loss_weight_1\": tune.grid_search([100]),\n",
    "    \"loss_weight_2\": tune.grid_search([1]),\n",
    "    \"loss_weight_3\": tune.grid_search([10]),\n",
    "    \"fc_width1\": tune.grid_search([256]),\n",
    "    \"fc_width2\": tune.grid_search([256]),\n",
    "    \"fc_width3\": tune.grid_search([256]),\n",
    "    \"rescale_1\": tune.grid_search([\"mean\"]),\n",
    "    \"rescale_2\": tune.grid_search([\"mean\"]),\n",
    "    \"rescale_3\": tune.grid_search([\"mean\"]),\n",
    "    \"LAE_weight\": tune.grid_search([2,3,4,5,6,7,8,9,10])\n",
    "    # \"momentum\": tune.grid_search([1.1]),\n",
    "    # \"weight_decay\": tune.grid_search([0])\n",
    "}\n",
    "\n",
    "# parser = argparse.ArgumentParser(description = \"DESSI_LAE\")\n",
    "# parser.add_argument(\n",
    "#     \"--cuda\", action = \"store_true\", default = False, help = \"Enables GPU training\"\n",
    "# )\n",
    "\n",
    "# args, _ = parser.parse_known_args()\n",
    "\n",
    "# ray.init(ignore_reinit_error = True)\n",
    "\n",
    "# sched = AsyncHyperBandScheduler()\n",
    "\n",
    "# resources_per_trial = {\"gpu\": 1}  # set this for GPUs\n",
    "# tuner = tune.Tuner(\n",
    "#     tune.with_resources(build_model, resources = resources_per_trial),\n",
    "#     tune_config = tune.TuneConfig(\n",
    "#         metric = \"mean_accuracy\",\n",
    "#         mode = \"max\",\n",
    "#         scheduler = sched,\n",
    "#         num_samples = 5,\n",
    "#     ),\n",
    "#     param_space = search_space\n",
    "# )\n",
    "    \n",
    "# results = tuner.fit()\n",
    "\n",
    "trainable_with_gpu = tune.with_resources(build_model, {\"gpu\": 4})\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_gpu,\n",
    "    param_space = search_space,\n",
    "    tune_config = tune.TuneConfig(num_samples = 5)\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "# tuner = tune.Tuner(build_model, param_space = search_space, tune_config = tune.TuneConfig(num_samples = 5, scheduler = sched))\n",
    "# results = tuner.fit()\n",
    "\n",
    "# analysis = tune.run(build_model, config = search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "R_H0efrzpNuz",
    "outputId": "ab40c624-6135-4280-ae39-c7d4fd731163",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(f\"../DESI_LAE_model/model_0.979_0.979_0.881_2024-02-07 07:13:17.397448.pt\")\n",
    "output, refer, diff = test_metrics(model, plot_wrong = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "08Sor68hhBGX",
    "outputId": "435affd2-87f2-4f0c-8246-a14dc0086e46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss_iter(test_loss = [i.item() for i in test_loss], train_loss = [i.item() for i in train_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "4hLwGjgTiZ7g",
    "outputId": "9e407f5b-8150-4686-d572-0bb262066fd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_test_accuracy(test_acc1 = test_acc_1, test_acc2 = test_acc_2, test_acc3 = test_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L7xhea6rY-Ls",
    "outputId": "9032f088-28b2-4ac9-ddea-fced063667c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(output1_record_train) - 1):\n",
    "  plt.close()\n",
    "  plt.figure(figsize = (16, 5))\n",
    "  plot_scatter(x = output1_record_train[i][1].to(\"cpu\"), y = output1_record_train[i][0].to(\"cpu\"), c = \"m\", ylabel = \"Predicted Position\", xlabel = \"True Position\", position = 1)\n",
    "  plot_scatter(x = output2_record_train[i][1].to(\"cpu\"), y = output2_record_train[i][0].to(\"cpu\"), c = \"m\", ylabel = \"Predicted Percentage\", xlabel = \"True Percentage\", position = 2)\n",
    "  plot_scatter(x = output3_record_train[i][1].to(\"cpu\"), y = output3_record_train[i][0].to(\"cpu\"), c = \"m\", ylabel = \"Predicted FWHM\", xlabel = \"True FWHM\", position = 3)\n",
    "\n",
    "  plot_scatter(x = output1_record_test[i][1].to(\"cpu\"), y = output1_record_test[i][0].to(\"cpu\"), c = \"y\", ylabel = \"Predicted Position\", xlabel = \"True Position\", position = 1)\n",
    "  plot_scatter(x = output2_record_test[i][1].to(\"cpu\"), y = output2_record_test[i][0].to(\"cpu\"), c = \"y\", ylabel = \"Predicted Percentage\", xlabel = \"True Percentage\", position = 2)\n",
    "  plot_scatter(x = output3_record_test[i][1].to(\"cpu\"), y = output3_record_test[i][0].to(\"cpu\"), c = \"y\", ylabel = \"Predicted FWHM\", xlabel = \"True FWHM\", position = 3)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcgnNE-3dQE9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_cropped2 = check_LAE(model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bSvkDC16noR8",
    "outputId": "570a0dbf-66d5-4c57-b04a-5bde6a585e5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in x_cropped2:\n",
    "  plt.close()\n",
    "  plt.plot(i[0].to(\"cpu\"))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia5ZyM9IkAOs"
   },
   "outputs": [],
   "source": [
    "spectra_train, spectra_test = train_test_split(LAE_spectra, test_size = 0.2, random_state = 2)\n",
    "x_range = np.linspace(0, 2438, x_cropped2.shape[-1])\n",
    "for i in range(0, x_cropped2.shape[0]):\n",
    "  plt.close()\n",
    "  plt.rcParams['figure.figsize'] = [16, 4]\n",
    "  plt.plot(x_range, x_scale_1[i][0].to(\"cpu\"), c = \"k\", lw = 1)\n",
    "  plt.plot(spectra_test[i][\"flux\"], alpha = 0.5, c = \"r\", lw = 1)\n",
    "  plt.title(spectra_test[i]['specid'])\n",
    "  plt.savefig(\"/content/drive/MyDrive/CNN/result/x_scale_1/\" + str(spectra_test[i]['specid']) + \".jpg\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGe7N8eZqjzU"
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, len(x_cropped[0][0]), x_scale_3.shape[-1])\n",
    "for i in range(0, x_scale_3.shape[0]):\n",
    "  plt.close()\n",
    "  plt.rcParams['figure.figsize'] = [16, 4]\n",
    "  # plt.plot(x_range, x_cropped[i][0].to(\"cpu\"), c = \"k\", lw = 1, alpha = 0.5)\n",
    "  # plt.plot(x_range, x_cropped[i][1].to(\"cpu\"), c = \"r\", lw = 1, alpha = 0.5)\n",
    "  plt.plot(x_range, x_scale_2[i][0].to(\"cpu\"), c = \"k\", lw = 1)\n",
    "  plt.plot(x_range, x_scale_2[i][1].to(\"cpu\"), c = \"r\", lw = 1)\n",
    "  plt.title(spectra_test[i]['specid'])\n",
    "  plt.savefig(\"/content/drive/MyDrive/CNN/result/x_scale_3/\" + str(spectra_test[i]['specid']) + \".jpg\")\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
