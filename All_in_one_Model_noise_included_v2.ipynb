{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "rpYGw1etWG8s",
    "outputId": "ad14e6e2-e5e8-4715-f01d-710e2d177372",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nersc_tensorboard_helper\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import pickle\n",
    "\n",
    "# from astropy.io import fits\n",
    "# from astropy.convolution import convolve, Gaussian1DKernel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5sA9GgKwWWMM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, x, y, target_transform = False):\n",
    "    self.labels = y\n",
    "    self.spectrum = x\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    spectra = self.spectrum[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    if self.target_transform != False:\n",
    "      label = self.target_transform(label)\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "    return torch.tensor(spectra), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3ZgxYDifiI-r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "  def __init__(self, weight = [1, 1, 1], punish_distance = False, punish_Lya = False):\n",
    "    super(customLoss, self).__init__()\n",
    "    self.weight = weight\n",
    "    self.punish_distance = punish_distance\n",
    "    self.punish_Lya = punish_Lya\n",
    "\n",
    "  def forward(self, output, target):\n",
    "    weight = self.weight\n",
    "    target = target.permute(1, 0, 2)\n",
    "\n",
    "    if self.punish_distance == True:\n",
    "      punish = (torch.abs(torch.argmax(output[0], dim = 1) - torch.argmax(target[0], dim = 1)) * torch.sum(target[0], dim = 1) + (output[0] > 0.1).sum(dim = 1) * (1 - torch.sum(target[0], dim = 1)))\n",
    "      punish = (punish / max(torch.sum(punish).item(), 1) * 64).unsqueeze(dim = 1).tile(1, 78)\n",
    "      BCE = nn.BCELoss(reduction = \"mean\", weight = punish)\n",
    "    elif self.punish_Lya != False:\n",
    "      punish = torch.where(torch.sum(target[0], dim = -1) > 0, self.punish_Lya, 1)\n",
    "      punish = ((punish / torch.sum(punish).item()) * 64).unsqueeze(dim = -1).tile(1, 78)\n",
    "      BCE = nn.BCELoss(reduction = \"mean\", weight = punish)\n",
    "    else:\n",
    "      BCE = nn.BCELoss(reduction = \"mean\")\n",
    "\n",
    "    MSE = nn.MSELoss(reduction = \"mean\")\n",
    "\n",
    "    loss_1 = BCE(output[0], target[0])\n",
    "    loss_2 = MSE(torch.sum(output[1] * target[0], dim = -1), torch.sum(target[1], dim = -1))\n",
    "    loss_3 = MSE(torch.sum(output[2] * target[0], dim = -1), torch.sum(target[2], dim = -1))\n",
    "\n",
    "    loss_total = weight[0] * loss_1 + weight[1] * loss_2 + weight[2] * loss_3\n",
    "\n",
    "    return loss_total / sum(weight), loss_1, loss_2, loss_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XkfeAzs3BCdB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\tdef __init__(self, kernel_size1_1, kernel_size1_4, kernel_size3_1, kernel_size3_2, kernel_size4_1, kernel_size4_2, ratio, out_channel7, pooling, fc_width1, fc_width2, fc_width3, rescale_1, rescale_2, rescale_3):\n",
    "\t\tself.rescale_1 = rescale_1\n",
    "\t\tself.rescale_2 = rescale_2\n",
    "\t\tself.rescale_3 = rescale_3\n",
    "\n",
    "\t\tsuper(ConvNet, self).__init__()\n",
    "\t\tkernel_size1_1 = kernel_size1_1\n",
    "\t\tkernel_size1_2 = round(kernel_size1_1 * ratio)\n",
    "\t\tkernel_size1_3 = round(kernel_size1_1 * ratio ** 2)\n",
    "\t\tkernel_size1_4 = kernel_size1_4\n",
    "\t\tstride = 1\n",
    "\t\tpadding1_1 = kernel_size1_1 // 2\n",
    "\t\tpadding1_2 = kernel_size1_2 // 2\n",
    "\t\tpadding1_3 = kernel_size1_3 // 2\n",
    "\t\tpadding1_4 = kernel_size1_4 // 2\n",
    "\t\tpadding3_1 = kernel_size3_1 // 2\n",
    "\t\tpadding3_2 = kernel_size3_2 // 2\n",
    "\t\tpadding4_1 = kernel_size4_1 // 2\n",
    "\t\tpadding4_2 = kernel_size4_2 // 2\n",
    "\t\tdilation = 1\n",
    "\t\tfactor = 2\n",
    "\t\tout_channel = 10 * factor\n",
    "\t\tout_channel2 = 10 * factor\n",
    "\t\tout_channel3 = 20 * factor\n",
    "\t\tout_channel4 = 20 * factor\n",
    "\t\tout_channel5 = 40 * factor\n",
    "\t\tout_channel6 = 80 * factor\n",
    "\t\tout_channel7 = out_channel7\n",
    "\n",
    "\t\tself.layer1_1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, out_channel, kernel_size = kernel_size1_1, stride = stride, padding = padding1_1, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel, out_channel2, kernel_size = kernel_size1_2, stride = stride, padding = padding1_2, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_3 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel2, out_channel3, kernel_size = kernel_size1_3, stride = stride, padding = padding1_3, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool1d(kernel_size = pooling, stride = pooling - 1, padding = pooling // 2),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_4 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel3, out_channel4, kernel_size = kernel_size1_4, stride = stride, padding = padding1_4, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel4),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool1d(kernel_size = pooling, stride = pooling - 1, padding = pooling // 2),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_5 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel4, out_channel5, kernel_size = 1, stride = stride, padding = 0, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel5),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_6 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel5, out_channel6, kernel_size = 1, stride = stride, padding = 0, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel6),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer1_7 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel6, out_channel7, kernel_size = 1, stride = stride, padding = 0, dilation = dilation),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdimension = math.floor(math.floor(2436 / pooling) / pooling) * out_channel7\n",
    "\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\tnn.Linear(610 * out_channel7, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, fc_width1),\n",
    "\t\t\tnn.BatchNorm1d(fc_width1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width1, 78),\n",
    "\t \t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer3_1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, out_channel, kernel_size = kernel_size3_1, stride = stride, padding = padding3_1, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer3_2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel, out_channel2, kernel_size = kernel_size3_2, stride = stride, padding = padding3_2, dilation = dilation),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer4_1 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, out_channel, kernel_size = 3, stride = stride, padding = 1, dilation = dilation),\n",
    "\t\t\tnn.BatchNorm1d(out_channel),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t)\n",
    "\t\tself.layer4_2 = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(out_channel, out_channel2, kernel_size = 3, stride = stride, padding = 1, dilation = dilation),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer5 = nn.Sequential(\n",
    "\t\t\tnn.Linear(1900, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, fc_width2),\n",
    "\t\t\tnn.BatchNorm1d(fc_width2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width2, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer6 = nn.Sequential(\n",
    "\t\t\tnn.Linear(900, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, fc_width3),\n",
    "\t\t\tnn.BatchNorm1d(fc_width3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_width3, 1),\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif self.rescale_1 == \"mean\":\n",
    "\t\t\tx_scale_1 = x - torch.mean(x, dim = -1, keepdim = True)\n",
    "\t\telif self.rescale_1 == \"zscore\":\n",
    "\t\t\tx_scale_1 = (x - torch.mean(x, dim = -1, keepdim = True)) / torch.std(x, dim = -1, keepdim = True)\n",
    "\t\telse:\n",
    "\t\t\tx_scale_1 = torch.clone(x)\n",
    "\n",
    "\t\tx1_1 = self.layer1_1(x_scale_1)\n",
    "\t\tx1_2 = self.layer1_2(x1_1)\n",
    "\t\tx1_3 = self.layer1_3(x1_2)\n",
    "\t\tx1_4 = self.layer1_4(x1_3)\n",
    "\t\tx1_5 = self.layer1_5(x1_4)\n",
    "\t\tx1_6 = self.layer1_6(x1_5)\n",
    "\t\tx1_7 = self.layer1_7(x1_6)\n",
    "\t\tx1_flatten = x1_7.view(x1_7.size(0), -1)\n",
    "\t\tx2 = self.layer2(x1_flatten)\n",
    "\t\tx2_mask = torch.zeros_like(x2).to(device)\n",
    "\t\tx2_mask[x2 > 0.7] = 1\n",
    "\n",
    "\t\tcrop_index1 = torch.round(torch.argmax(x2, dim = -1) * 31.25 + 15.625).unsqueeze(1) + torch.arange(-47, 48).to(device) + 48\n",
    "\t\tcrop_index1 = crop_index1.type(torch.long).to(device)\n",
    "\t\tx = x.squeeze(dim = 1)\n",
    "\t\tx_extended = torch.cat((x, torch.full_like(x[:, :48], torch.median(x)).to(device)), dim = -1)\n",
    "\t\tx_extended = torch.cat((torch.full_like(x[:, :48], torch.median(x)).to(device), x_extended), dim = 1)\n",
    "\t\tx_cropped = x_extended[torch.arange(x_extended.size(0)).unsqueeze(1).to(device), crop_index1]\n",
    "\t\tx_cropped = x_cropped.unsqueeze(dim = 1)\n",
    "\t\tx = x.unsqueeze(dim = 1)\n",
    "\n",
    "\t\tif self.rescale_2 == \"mean\":\n",
    "\t\t\tx_scale_2 = x_cropped - torch.mean(x_cropped, dim = -1, keepdim = True)\n",
    "\t\telif self.rescale_2 == \"zscore\":\n",
    "\t\t\tx_scale_2 = (x_cropped - torch.mean(x_cropped, dim = -1, keepdim = True)) / torch.std(x_cropped, dim = -1, keepdim = True)\n",
    "\t\telse:\n",
    "\t\t\tx_scale_2 = torch.clone(x_cropped)\n",
    "\n",
    "\t\tx3_1 = self.layer3_1(x_scale_2)\n",
    "\t\tx3_2 = self.layer3_2(x3_1)\n",
    "\t\tx3_flatten = x3_2.view(x3_2.size(0), -1)\n",
    "\n",
    "\t\tx5 = self.layer5(x3_flatten)\n",
    "\t\tcrop_index2 = torch.round(torch.sum(x5, dim = -1) * x_cropped.size(-1)).unsqueeze(1) + torch.arange(-22, 23).to(device) + 23\n",
    "\t\tx5 = x2_mask * x5\n",
    "\n",
    "\t\tcrop_index2 = crop_index2.type(torch.long).to(device)\n",
    "\t\tx_cropped = x_cropped.squeeze(dim = 1)\n",
    "\t\tx_cropped2 = torch.cat((torch.full_like(x_cropped[:, :23], torch.median(x_cropped)).to(device), x_cropped, torch.full_like(x_cropped[:, :23], torch.median(x_cropped)).to(device)), dim = -1)\n",
    "\t\tx_cropped2 = x_cropped2[torch.arange(x_cropped2.size(0)).unsqueeze(1).to(device), crop_index2]\n",
    "\t\tx_cropped2 = x_cropped2.unsqueeze(dim = 1)\n",
    "\t\tx_cropped = x_cropped.unsqueeze(dim = 1)\n",
    "\n",
    "\t\tif self.rescale_3 == \"mean\":\n",
    "\t\t\tx_scale_3 = x_cropped2 - torch.mean(x_cropped2, dim = -1, keepdim = True)\n",
    "\t\telif self.rescale_3 == \"zscore\":\n",
    "\t\t\tx_scale_3 = (x_cropped2 - torch.mean(x_cropped2, dim = -1, keepdim = True)) / torch.std(x_cropped2, dim = -1, keepdim = True)\n",
    "\t\telse:\n",
    "\t\t\tx_scale_3 = torch.clone(x_cropped2)\n",
    "\n",
    "\t\tx4_1 = self.layer4_1(x_scale_3)\n",
    "\t\tx4_2 = self.layer4_2(x4_1)\n",
    "\t\tx4_flatten = x4_2.view(x4_2.size(0), -1)\n",
    "\n",
    "\t\tx6 = self.layer6(x4_flatten)\n",
    "\t\tx6 = x2_mask * x6\n",
    "\n",
    "\t\tx2 = torch.unsqueeze(x2, dim = 0)\n",
    "\t\tx5 = torch.unsqueeze(x5, dim = 0)\n",
    "\t\tx6 = torch.unsqueeze(x6, dim = 0)\n",
    "\n",
    "\t\treturn torch.cat((x2, x5, x6), 0), x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "thksTRyeMvIG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spectra_info = []\n",
    "\n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_spectra/fuji_pre_b.pkl\", \"rb\") as fh:\n",
    "#     spectra = pickle.load(fh)\n",
    "#     spectra_info += spectra\n",
    "#     pre = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in spectra]) * np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in spectra])\n",
    "\n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_spectra/fuji_similar_b.pkl\", \"rb\") as fh:\n",
    "#     spectra = pickle.load(fh)\n",
    "#     spectra_info += spectra\n",
    "#     similar = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in spectra]) * np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in spectra])\n",
    "\n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_spectra/fuji_NLAE_b.pkl\", \"rb\") as fh:\n",
    "#     spectra = pickle.load(fh)\n",
    "#     spectra_info += spectra\n",
    "#     NLAE = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in spectra]) * np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in spectra])\n",
    "    \n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_spectra/fuji_random_b.pkl\", \"rb\") as fh:\n",
    "#     spectra = pickle.load(fh)\n",
    "#     spectra_info += spectra\n",
    "#     random = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in spectra]) * np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in spectra])\n",
    "    \n",
    "# np.random.seed(3)\n",
    "# np.random.shuffle(spectra_info)\n",
    "\n",
    "# combine1 = np.concatenate((pre, similar, NLAE, random), axis = 0)\n",
    "# np.random.seed(3)\n",
    "# np.random.shuffle(combine1)\n",
    "\n",
    "# # x_train = np.concatenate((pre, similar, NLAE, random), axis = 0)\n",
    "# x_train, x_test = train_test_split(combine1, test_size = 0.2, random_state = 2)\n",
    "\n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_label/fuji_pre_25.pkl\", \"rb\") as fh:\n",
    "#     pre_label = pickle.load(fh)\n",
    "\n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_label/fuji_similar_25.pkl\", \"rb\") as fh:\n",
    "#     similar_label = pickle.load(fh)\n",
    "    \n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_label/fuji_NLAE_25.pkl\", \"rb\") as fh:\n",
    "#     NLAE_label = pickle.load(fh)\n",
    "\n",
    "# with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_label/fuji_random_25.pkl\", \"rb\") as fh:\n",
    "#     random_label = pickle.load(fh)\n",
    "\n",
    "# combine2 = np.concatenate((pre_label, similar_label, NLAE_label, random_label), axis = 0)\n",
    "# np.random.seed(3)\n",
    "# np.random.shuffle(combine2)\n",
    "                                   \n",
    "# y_train, y_test = train_test_split(combine2, test_size = 0.2, random_state = 2)\n",
    "\n",
    "spectra_info_test = []\n",
    "with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_spectra/iron_pre_b.pkl\", \"rb\") as fh:\n",
    "    spectra = pickle.load(fh)\n",
    "    spectra_info_test += spectra\n",
    "    iron = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in spectra]) * np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in spectra])\n",
    "    \n",
    "np.random.seed(3)\n",
    "np.random.shuffle(spectra_info_test)\n",
    "\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(iron)\n",
    "x_train, x_test = train_test_split(iron, test_size = 0.2, random_state = 2)\n",
    "    \n",
    "with open(\"/pscratch/sd/j/juikuan/DESI_LAE_dataset/train_label/iron_pre_25.pkl\", \"rb\") as fh:\n",
    "    iron_label = pickle.load(fh)\n",
    "\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(iron_label)\n",
    "y_train, y_test = train_test_split(iron_label, test_size = 0.2, random_state = 2)\n",
    "\n",
    "x_train_id = ray.put(x_train)\n",
    "y_train_id = ray.put(y_train)\n",
    "x_test_id = ray.put(x_test)\n",
    "y_test_id = ray.put(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kgP3VX3BW1kj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, criterion, train_loader):\n",
    "    train_loss_iter = 0\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    correct_3 = 0\n",
    "    output1_record = torch.tensor([[], []])\n",
    "    output2_record = torch.tensor([[], []])\n",
    "    output3_record = torch.tensor([[], []])\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model = model.to(device).float()\n",
    "        output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data.float())\n",
    "\n",
    "        loss, loss_1, loss_2, loss_3 = criterion(output.float(), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_iter += loss\n",
    "\n",
    "        output = output.detach()\n",
    "\n",
    "        output1 = output[0].argmax(dim = -1)\n",
    "        output1[output[0].max(dim = -1)[0] < 0.7] = 0\n",
    "        target1 = target.permute(1, 0, 2)[0].argmax(dim = -1)\n",
    "        difference1 = (output1 - target1) ** 2\n",
    "        correct_1 += len(difference1[difference1 < 1])\n",
    "        output1_record = torch.cat((output1_record.to(\"cpu\"), torch.cat((output1.unsqueeze(0).to(\"cpu\"), target1.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "        mask1 = torch.where(output[0] > 0.7, 1, 0)\n",
    "        # mask2 = torch.where(difference_1 > 0, -1, 1)\n",
    "\n",
    "        output2 = torch.sum(output[1] * mask1, dim = -1)\n",
    "        target2 = torch.sum((target.permute(1, 0, 2)[1]), dim = -1)\n",
    "        difference2 = (output2 - target2) ** 2\n",
    "        # difference_2 = difference_2 * mask2\n",
    "        # difference_2 = difference_2\n",
    "        # difference_2 = difference_2[difference_2 >= 0]\n",
    "        correct_2 += len(difference2[difference2 < 0.0025])\n",
    "        output2_record = torch.cat((output2_record.to(\"cpu\"), torch.cat((output2.unsqueeze(0).to(\"cpu\"), target2.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "        output3 = torch.sum(output[2] * mask1, dim = -1)\n",
    "        target3 = torch.sum((target.permute(1, 0, 2)[2]), dim = -1)\n",
    "        difference3 = (output3 - target3) ** 2\n",
    "        # difference_3 = difference_3 * mask2\n",
    "        # difference_3 = difference_3\n",
    "        # difference_3 = difference_3[difference_3 >= 0]\n",
    "        correct_3 += len(difference3[difference3 < 9])\n",
    "        output3_record = torch.cat((output3_record.to(\"cpu\"), torch.cat((output3.unsqueeze(0).to(\"cpu\"), target3.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "    train_loss_iter /= len(train_loader.dataset)\n",
    "\n",
    "    return model, train_loss_iter, output1_record, output2_record, output3_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lSJxXfdYfhjl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(epoch, model, criterion, test_loader, ray_tune):\n",
    "    test_loss_iter = 0\n",
    "    test_loss1_iter = 0\n",
    "    test_loss2_iter = 0\n",
    "    test_loss3_iter = 0\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    correct_3 = 0\n",
    "    output1_record = torch.tensor([[], []])\n",
    "    output2_record = torch.tensor([[], []])\n",
    "    output3_record = torch.tensor([[], []])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            model = model.to(device).float()\n",
    "            output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data.float())\n",
    "\n",
    "            loss, loss_1, loss_2, loss_3 = criterion(output.float(), target.float())\n",
    "            test_loss_iter += loss\n",
    "            test_loss1_iter += loss_1\n",
    "            test_loss2_iter += loss_2\n",
    "            test_loss3_iter += loss_3\n",
    "\n",
    "            output1 = output[0].argmax(dim = -1)\n",
    "            output1[output[0].max(dim = -1)[0] < 0.7] = 0\n",
    "            target1 = target.permute(1, 0, 2)[0].argmax(dim = -1)\n",
    "            difference1 = (output1 - target1) ** 2\n",
    "            correct_1 += len(difference1[difference1 < 1])\n",
    "            output1_record = torch.cat((output1_record.to(\"cpu\"), torch.cat((output1.unsqueeze(0).to(\"cpu\"), target1.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "\n",
    "            mask1 = torch.where(output[0] > 0.7, 1, 0)\n",
    "            # mask2 = torch.where(difference_1 > 0, -1, 1)\n",
    "\n",
    "            output2 = torch.sum(output[1] * mask1, dim = -1)\n",
    "            target2 = torch.sum((target.permute(1, 0, 2)[1]), dim = -1)\n",
    "            difference2 = (output2 - target2) ** 2\n",
    "            # difference_2 = difference_2 * mask2\n",
    "            # difference_2 = difference_2\n",
    "            # difference_2 = difference_2[difference_2 >= 0]\n",
    "            correct_2 += len(difference2[difference2 < 0.0025])\n",
    "            output2_record = torch.cat((output2_record.to(\"cpu\"), torch.cat((output2.unsqueeze(0).to(\"cpu\"), target2.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "            output3 = torch.sum(output[2] * mask1, dim = -1)\n",
    "            target3 = torch.sum((target.permute(1, 0, 2)[2]), dim = -1)\n",
    "            difference3 = (output3 - target3) ** 2\n",
    "            # difference_3 = difference_3 * mask2\n",
    "            # difference_3 = difference_3\n",
    "            # difference_3 = difference_3[difference_3 >= 0]\n",
    "            correct_3 += len(difference3[difference3 < 9])\n",
    "            output3_record = torch.cat((output3_record.to(\"cpu\"), torch.cat((output3.unsqueeze(0).to(\"cpu\"), target3.unsqueeze(0).to(\"cpu\")), dim = 0)), dim = -1)\n",
    "\n",
    "    data_count = len(test_loader.dataset)\n",
    "\n",
    "    test_loss_iter /= data_count\n",
    "    test_loss1_iter /= data_count\n",
    "    test_loss2_iter /= data_count\n",
    "    test_loss3_iter /= data_count\n",
    "\n",
    "    if not ray_tune:\n",
    "        print(f'平均損失: {test_loss_iter:.6f}, 1st loss: {test_loss1_iter:.6f}({correct_1 / data_count * 100:.1f}), 2nd loss: {test_loss2_iter:.6f}({correct_2 / data_count * 100:.1f}), 3rd loss: {test_loss3_iter:.6f}({correct_3 / data_count * 100:.1f})')\n",
    "\n",
    "    return model, test_loss_iter, [correct_1 / data_count, correct_2 / data_count, correct_3 / data_count], output1_record, output2_record, output3_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TqShYOTHJS8u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity = 'relu', mode = 'fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-48MBtWOXT66",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(config, reporter, x_train_id = x_train_id, y_train_id = y_train_id, x_test_id = x_test_id, y_test_id = y_test_id, ray_tune = True, model_size = \"M\"):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_acc_1 = []\n",
    "    test_acc_2 = []\n",
    "    test_acc_3 = []\n",
    "    output1_record_train = []\n",
    "    output2_record_train = []\n",
    "    output3_record_train = []\n",
    "    output1_record_test = []\n",
    "    output2_record_test = []\n",
    "    output3_record_test = []\n",
    "    \n",
    "    \n",
    "    x = ray.get(x_train_id)\n",
    "    x = x[0: round(config[\"training/ratio\"] * len(x))]\n",
    "    \n",
    "    y = ray.get(y_train_id)\n",
    "    y = y[0: round(config[\"training/ratio\"] * len(y))]\n",
    "    \n",
    "    train_dataset = CustomDataset(x = x, y = y)\n",
    "    test_dataset = CustomDataset(x = ray.get(x_test_id), y = ray.get(y_test_id))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config[\"training/batch\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size = config[\"training/batch\"])\n",
    "\n",
    "    model = ConvNet(kernel_size1_1 = config[\"model/kernal/ks1_1\"], kernel_size4_2 = config[\"model/kernal/ks4_2\"],\n",
    "                  kernel_size1_4 = config[\"model/kernal/ks1_4\"], kernel_size3_1 = config[\"model/kernal/ks3_1\"],\n",
    "                  kernel_size3_2 = config[\"model/kernal/ks3_2\"], kernel_size4_1 = config[\"model/kernal/ks4_1\"],\n",
    "                  ratio = config[\"model/kernal/ratio\"], out_channel7 = config[\"model/width/out_channel7\"],\n",
    "                  pooling = config[\"model/pooling\"], fc_width1 = config[\"model/width/w1\"],\n",
    "                  fc_width2 = config[\"model/width/w2\"], fc_width3 = config[\"model/width/w3\"],\n",
    "                  rescale_1 = config[\"data/re1\"], rescale_2 = config[\"data/re2\"],\n",
    "                  rescale_3 = config[\"data/re3\"])\n",
    "\n",
    "    criterion = customLoss(weight = [config[\"loss/l1\"], config[\"loss/l2\"], config[\"loss/l3\"]], punish_Lya = config[\"loss/LAE_w\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr = config[\"optim/lr\"])\n",
    "\n",
    "    epoch = 50\n",
    "\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    for i in range(1, epoch + 1):\n",
    "        # optimizer = optim.SGD(model.parameters(), lr = (config[\"lr\"] * (1 - i / 60)) ** (1 / 2),\n",
    "        #             momentum = config[\"momentum\"], weight_decay = config[\"weight_decay\"])\n",
    "\n",
    "        model.train()\n",
    "        model, train_loss_iter, output1_record, output2_record, output3_record = train(epoch = i, model = model, criterion = criterion, optimizer = optimizer, train_loader = train_loader)\n",
    "        train_loss.append(train_loss_iter)\n",
    "        output1_record_train.append(output1_record)\n",
    "        output2_record_train.append(output2_record)\n",
    "        output3_record_train.append(output3_record)\n",
    "\n",
    "        model.eval()\n",
    "        model, test_loss_iter, test_accuracy_iter, output1_record, output2_record, output3_record = test(epoch = i, model = model, criterion = criterion, test_loader = test_loader, ray_tune = ray_tune)\n",
    "        test_loss.append(test_loss_iter)\n",
    "        test_acc_1.append(test_accuracy_iter[0])\n",
    "        test_acc_2.append(test_accuracy_iter[1])\n",
    "        test_acc_3.append(test_accuracy_iter[2])\n",
    "        output1_record_test.append(output1_record)\n",
    "        output2_record_test.append(output2_record)\n",
    "        output3_record_test.append(output3_record)\n",
    "\n",
    "        if ray_tune:\n",
    "            reporter(mean_accuracy = test_acc_1[test_loss.index(min(test_loss))], mean_loss = test_loss[-1].item())\n",
    "        else:\n",
    "            if test_accuracy_iter[0] > 0.9:\n",
    "                current_time = datetime.datetime.now()\n",
    "                torch.save(model, f\"/pscratch/sd/j/juikuan/DESI_LAE_model/model_{test_accuracy_iter[0]:.3f}_{test_accuracy_iter[1]:.3f}_{test_accuracy_iter[2]:.3f}_{current_time}.pt\")\n",
    "                print(\"Saved!\")\n",
    "                \n",
    "    if not ray_tune:\n",
    "        return model, train_loss, test_loss, test_acc_1, test_acc_2, test_acc_3, output1_record_train, output2_record_train, output3_record_train, output1_record_test, output2_record_test, output3_record_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_iter(test_loss, train_loss):\n",
    "    plt.close()\n",
    "    epoch = [i for i in range(1, len(test_loss) + 1)]\n",
    "    plt.plot(epoch, test_loss, label = \"Test Loss\", c = \"m\")\n",
    "    plt.plot(epoch, train_loss, label = \"Train Loss\", c = \"y\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    # plt.savefig(\"/content/drive/MyDrive/loss.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_accuracy(test_acc1, test_acc2, test_acc3):\n",
    "    plt.close()\n",
    "    epoch = [i for i in range(1, len(test_acc1) + 1)]\n",
    "    plt.plot(epoch, test_acc1, label = \"Rough Position\", c = \"r\")\n",
    "    plt.plot(epoch, test_acc2, label = \"Precise Position\", c = \"m\")\n",
    "    plt.plot(epoch, test_acc3, label = \"FWHM\", c = \"y\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    # plt.savefig(\"/content/drive/MyDrive/loss.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "3jJJoFl5cak5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_perform(model, spectra, label, spectra_info, plot_wrong = False):\n",
    "    with torch.no_grad():\n",
    "        data = torch.tensor(spectra).to(device).float()\n",
    "        model = model.to(device).float()\n",
    "        output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data)\n",
    "        output = output.to(\"cpu\")\n",
    "    \n",
    "    print(output.shape)\n",
    "    mask1 = torch.where(output[0] > 0.7, 1, 0)\n",
    "    a = torch.argmax(mask1, axis = -1)\n",
    "    b = torch.argmax(torch.tensor(label).permute(1, 0, 2)[0], axis = -1)\n",
    "    diff = torch.argwhere(a - b).squeeze(dim = -1)\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "\n",
    "    x_range = spectra_info[0][\"WAVE\"]\n",
    "    pixel_range = np.array([i * 25 + 3600 for i in range(0, 78)])\n",
    "    \n",
    "    plt.close()\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    plt.figure(dpi = 200)\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(x = a, y = b, c = \"m\", s = 5)\n",
    "    plt.xlabel(xlabel = \"Predicted Position\")\n",
    "    plt.ylabel(ylabel = \"True Position\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.scatter(x = torch.sum(mask1 * output[1], dim = -1), y = torch.sum(torch.tensor(label).permute(1, 0, 2)[1], axis = -1), c = \"m\", s = 5)\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100) - 0.05, c = \"k\")\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100) + 0.05, c = \"k\")\n",
    "    plt.xlabel(xlabel = \"Predicted Percentage\")\n",
    "    plt.ylabel(ylabel = \"True Percentage\")\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.scatter(x = torch.sum(mask1 * output[2], dim = -1), y = torch.sum(torch.tensor(label).permute(1, 0, 2)[2], axis = -1), c = \"m\", s = 5)\n",
    "    plt.plot(np.linspace(0, 40, 100), np.linspace(0, 40, 100) - 3, c = \"k\")\n",
    "    plt.plot(np.linspace(0, 40, 100), np.linspace(0, 40, 100) + 3, c = \"k\")\n",
    "    plt.xlabel(xlabel = \"Predicted Width (pixel)\")\n",
    "    plt.ylabel(ylabel = \"True Width (pixel)\")\n",
    "    \n",
    "    plt.savefig(f\"/pscratch/sd/j/juikuan/DESI_LAE_diagram/scatter_{current_time}.jpg\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # for i in diff:\n",
    "    #     print(\"=======================================================================================================================\")\n",
    "    #     if (a[i].item() == 0) & (b[i].item() != 0):\n",
    "    #         print(\"Ignored LAE.\")\n",
    "    #     elif (a[i].item() != 0) & (b[i].item() == 0):\n",
    "    #         print(\"NLAE detected.\")\n",
    "    #     elif (a[i] - b[i]).item() ** 2 == 1:\n",
    "    #         print(\"Nearby offset.\")\n",
    "    #     elif (a[i] - b[i]).item() ** 2 > 1:\n",
    "    #         print(\"Offset.\")\n",
    "    #     print(f\"Prediction: {a[i].item()}\")\n",
    "    #     print(f\"True: {b[i].item()}\")\n",
    "    #     print(output[0][i])\n",
    "    #     print(torch.tensor(label).permute(1, 0, 2)[0][i])\n",
    "        \n",
    "    if plot_wrong:\n",
    "        plt.close()\n",
    "        plt.rcParams['figure.figsize'] = [16, 4]\n",
    "        plt.plot(x_range, x_scale_1[i][0].to(\"cpu\"), alpha = 1, c = \"r\", lw = 1)\n",
    "        plt.bar(pixel_range, output[0][i].to(\"cpu\") * torch.max(x_scale_1[i][0].to(\"cpu\")).item(), width = 25, align = \"edge\")\n",
    "        plt.title(str(spectra_info[i]['specid']))\n",
    "        print(str(spectra_info[i]['specid']))\n",
    "        # plt.savefig(\"/content/drive/MyDrive/CNN/result/x_scale_3/\" + str(spectra_info[i]['specid']) + \".jpg\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    cm = confusion_matrix(y_pred = torch.max(mask1, dim = -1)[0], y_true = torch.sum(torch.tensor(label).permute(1, 0, 2)[0], dim = -1))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"NLAE\", \"LAE\"])\n",
    "    disp.plot()\n",
    "    # plt.figure(dpi = 200)\n",
    "    plt.savefig(f\"/pscratch/sd/j/juikuan/DESI_LAE_diagram/confusion_matrix_{current_time}.jpg\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "    return output, torch.tensor(label).permute(1, 0, 2), diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../DESI_LAE_dataset/train_spectra/LAE_iron_b.pkl\", \"rb\") as fh:\n",
    "    LAE_spectra = pickle.load(fh)\n",
    "    LAE_flux = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in LAE_spectra])\n",
    "    LAE_ivar = np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in LAE_spectra])\n",
    "    LAE = LAE_flux * LAE_ivar\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/train_spectra/NLAE_iron_b.pkl\", \"rb\") as fh:\n",
    "    NLAE_spectra = pickle.load(fh)\n",
    "    NLAE_flux = np.array([np.array(i[\"FLUX\"]).reshape(1, -1) for i in NLAE_spectra])\n",
    "    NLAE_ivar = np.array([np.array(i[\"IVAR\"]).reshape(1, -1) ** (1 / 2) for i in NLAE_spectra])\n",
    "    NLAE = NLAE_flux * NLAE_ivar\n",
    "    \n",
    "test_iron_spetra = np.concatenate((LAE, NLAE), axis = 0)\n",
    "info_spectra = LAE_spectra + NLAE_spectra\n",
    "    \n",
    "with open(\"../DESI_LAE_dataset/train_label/LAE_iron_25.pkl\", \"rb\") as fh:\n",
    "  LAE_label = pickle.load(fh)\n",
    "\n",
    "with open(\"../DESI_LAE_dataset/train_label/NLAE_iron_25.pkl\", \"rb\") as fh:\n",
    "  NLAE_label = pickle.load(fh)\n",
    "\n",
    "test_iron_label = np.concatenate((LAE_label, NLAE_label), axis = 0)\n",
    "\n",
    "model = torch.load(f\"../DESI_LAE_model/model_0.977_0.974_0.891_2024-02-07 12:05:21.648665.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f\"/pscratch/sd/j/juikuan/DESI_LAE_model/model_0.903_0.902_0.763_2024-04-17 02:27:49.833868.pt\")\n",
    "output, label, diff = test_perform(model = model, spectra = x_train, label = y_train, spectra_info = spectra_info, plot_wrong = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mb79cd0YnNrr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_LAE(model):\n",
    "  with torch.no_grad():\n",
    "    data = torch.tensor(LAE_flux_test).to(device).float()\n",
    "    model = model.to(device).float()\n",
    "    output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 = model(data)\n",
    "  return x_cropped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimal_perform(search_space):\n",
    "    n = 0\n",
    "    while True:\n",
    "        n += 1\n",
    "        print(\"=======================================================================================================================\")\n",
    "        print(f\"No.{n} trial:\")\n",
    "        model, train_loss, test_loss, test_acc_1, test_acc_2, test_acc_3, output1_record_train, output2_record_train, output3_record_train, output1_record_test, output2_record_test, output3_record_test = build_model(config = search_space, ray_tune = False, reporter = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zxjmJ44zHLVD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================\n",
      "No.1 trial:\n",
      "平均損失: 0.165238, 1st loss: 0.000636(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.164765, 1st loss: 0.000112(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.162057, 1st loss: 0.000055(47.6), 2nd loss: 0.000826(44.7), 3rd loss: 1.798199(45.0)\n",
      "平均損失: 0.151110, 1st loss: 0.000035(54.1), 2nd loss: 0.000792(42.2), 3rd loss: 1.676885(45.0)\n",
      "平均損失: 0.141732, 1st loss: 0.000028(59.5), 2nd loss: 0.000654(46.4), 3rd loss: 1.572885(45.6)\n",
      "平均損失: 0.120815, 1st loss: 0.000024(67.0), 2nd loss: 0.000512(46.4), 3rd loss: 1.340755(49.0)\n",
      "平均損失: 0.106330, 1st loss: 0.000023(70.9), 2nd loss: 0.000436(59.8), 3rd loss: 1.179988(52.7)\n",
      "平均損失: 0.165049, 1st loss: 0.000082(45.9), 2nd loss: 0.000840(45.9), 3rd loss: 1.831138(45.6)\n",
      "平均損失: 0.130950, 1st loss: 0.000026(66.4), 2nd loss: 0.000547(66.4), 3rd loss: 1.453234(51.3)\n",
      "平均損失: 0.067194, 1st loss: 0.000023(73.2), 2nd loss: 0.000198(73.2), 3rd loss: 0.745602(49.3)\n",
      "平均損失: 0.148903, 1st loss: 0.000046(55.8), 2nd loss: 0.000702(55.8), 3rd loss: 1.652290(52.1)\n",
      "平均損失: 0.102000, 1st loss: 0.000022(74.9), 2nd loss: 0.000344(74.9), 3rd loss: 1.131943(56.1)\n",
      "平均損失: 0.095196, 1st loss: 0.000021(77.2), 2nd loss: 0.000319(77.2), 3rd loss: 1.056435(61.8)\n",
      "平均損失: 0.105688, 1st loss: 0.000024(74.6), 2nd loss: 0.000363(74.6), 3rd loss: 1.172861(60.1)\n",
      "平均損失: 0.092367, 1st loss: 0.000021(76.6), 2nd loss: 0.000282(76.6), 3rd loss: 1.025033(54.1)\n",
      "平均損失: 0.087176, 1st loss: 0.000020(75.5), 2nd loss: 0.000266(75.5), 3rd loss: 0.967428(58.4)\n",
      "平均損失: 0.091716, 1st loss: 0.000020(75.8), 2nd loss: 0.000289(75.8), 3rd loss: 1.017816(61.0)\n",
      "平均損失: 0.091658, 1st loss: 0.000020(75.8), 2nd loss: 0.000294(75.8), 3rd loss: 1.017169(64.7)\n",
      "平均損失: 0.082870, 1st loss: 0.000020(76.4), 2nd loss: 0.000272(76.4), 3rd loss: 0.919631(55.0)\n",
      "平均損失: 0.078207, 1st loss: 0.000020(76.9), 2nd loss: 0.000253(76.9), 3rd loss: 0.867881(63.0)\n",
      "平均損失: 0.078469, 1st loss: 0.000020(76.9), 2nd loss: 0.000253(76.9), 3rd loss: 0.870780(63.5)\n",
      "平均損失: 0.081346, 1st loss: 0.000020(76.4), 2nd loss: 0.000266(76.4), 3rd loss: 0.902716(60.7)\n",
      "平均損失: 0.081754, 1st loss: 0.000020(76.1), 2nd loss: 0.000266(76.1), 3rd loss: 0.907251(62.4)\n",
      "平均損失: 0.080482, 1st loss: 0.000020(75.8), 2nd loss: 0.000266(75.8), 3rd loss: 0.893128(62.4)\n",
      "平均損失: 0.081177, 1st loss: 0.000020(75.8), 2nd loss: 0.000266(75.8), 3rd loss: 0.900845(63.0)\n",
      "平均損失: 0.079534, 1st loss: 0.000020(76.4), 2nd loss: 0.000252(76.4), 3rd loss: 0.882603(64.7)\n",
      "平均損失: 0.079316, 1st loss: 0.000020(76.6), 2nd loss: 0.000244(76.6), 3rd loss: 0.880185(65.5)\n",
      "平均損失: 0.078626, 1st loss: 0.000020(76.6), 2nd loss: 0.000244(76.6), 3rd loss: 0.872526(65.8)\n",
      "平均損失: 0.076674, 1st loss: 0.000020(76.9), 2nd loss: 0.000231(76.9), 3rd loss: 0.850866(64.1)\n",
      "平均損失: 0.075476, 1st loss: 0.000020(76.9), 2nd loss: 0.000227(76.9), 3rd loss: 0.837569(66.4)\n",
      "平均損失: 0.074188, 1st loss: 0.000020(77.2), 2nd loss: 0.000225(77.2), 3rd loss: 0.823271(64.7)\n",
      "平均損失: 0.073334, 1st loss: 0.000020(77.5), 2nd loss: 0.000223(77.5), 3rd loss: 0.813788(64.1)\n",
      "平均損失: 0.071104, 1st loss: 0.000020(78.1), 2nd loss: 0.000217(78.1), 3rd loss: 0.789038(68.1)\n",
      "平均損失: 0.071734, 1st loss: 0.000020(77.8), 2nd loss: 0.000217(77.8), 3rd loss: 0.796031(67.5)\n",
      "平均損失: 0.071513, 1st loss: 0.000020(78.3), 2nd loss: 0.000210(78.3), 3rd loss: 0.793575(65.0)\n",
      "平均損失: 0.070676, 1st loss: 0.000020(78.1), 2nd loss: 0.000210(78.1), 3rd loss: 0.784280(67.8)\n",
      "平均損失: 0.070703, 1st loss: 0.000020(78.1), 2nd loss: 0.000210(78.1), 3rd loss: 0.784580(66.7)\n",
      "平均損失: 0.070084, 1st loss: 0.000020(78.1), 2nd loss: 0.000210(78.1), 3rd loss: 0.777712(66.7)\n",
      "平均損失: 0.070751, 1st loss: 0.000020(78.1), 2nd loss: 0.000210(78.1), 3rd loss: 0.785113(65.2)\n",
      "平均損失: 0.071368, 1st loss: 0.000020(78.1), 2nd loss: 0.000210(78.1), 3rd loss: 0.791962(63.8)\n",
      "平均損失: 0.071861, 1st loss: 0.000020(78.1), 2nd loss: 0.000210(78.1), 3rd loss: 0.797435(62.7)\n",
      "平均損失: 0.070171, 1st loss: 0.000020(78.3), 2nd loss: 0.000206(78.3), 3rd loss: 0.778675(68.1)\n",
      "平均損失: 0.072715, 1st loss: 0.000020(78.6), 2nd loss: 0.000204(78.6), 3rd loss: 0.806919(61.8)\n",
      "平均損失: 0.071085, 1st loss: 0.000020(78.9), 2nd loss: 0.000196(78.9), 3rd loss: 0.788827(61.8)\n",
      "平均損失: 0.070692, 1st loss: 0.000020(78.9), 2nd loss: 0.000196(78.9), 3rd loss: 0.784461(62.7)\n",
      "平均損失: 0.069568, 1st loss: 0.000020(78.9), 2nd loss: 0.000196(78.9), 3rd loss: 0.771984(63.2)\n",
      "平均損失: 0.068863, 1st loss: 0.000020(78.9), 2nd loss: 0.000197(78.9), 3rd loss: 0.764162(65.2)\n",
      "平均損失: 0.071341, 1st loss: 0.000020(78.9), 2nd loss: 0.000196(78.9), 3rd loss: 0.791666(63.2)\n",
      "平均損失: 0.070227, 1st loss: 0.000020(78.9), 2nd loss: 0.000196(78.9), 3rd loss: 0.779296(67.5)\n",
      "平均損失: 0.071817, 1st loss: 0.000020(78.9), 2nd loss: 0.000197(78.9), 3rd loss: 0.796947(63.8)\n",
      "=======================================================================================================================\n",
      "No.2 trial:\n",
      "平均損失: 0.165300, 1st loss: 0.000706(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.164758, 1st loss: 0.000103(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.160117, 1st loss: 0.000056(47.6), 2nd loss: 0.000840(45.6), 3rd loss: 1.776660(44.7)\n",
      "平均損失: 0.152831, 1st loss: 0.000037(53.0), 2nd loss: 0.000839(45.6), 3rd loss: 1.695975(46.4)\n",
      "平均損失: 0.164750, 1st loss: 0.000095(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.177543, 1st loss: 0.000046(48.4), 2nd loss: 0.000783(47.0), 3rd loss: 1.970188(44.7)\n",
      "平均損失: 0.122266, 1st loss: 0.000027(58.1), 2nd loss: 0.000567(54.1), 3rd loss: 1.356821(48.7)\n",
      "平均損失: 0.150604, 1st loss: 0.000028(55.0), 2nd loss: 0.000711(55.0), 3rd loss: 1.671350(47.0)\n",
      "平均損失: 0.118096, 1st loss: 0.000023(67.8), 2nd loss: 0.000497(65.8), 3rd loss: 1.310590(52.7)\n",
      "平均損失: 0.142521, 1st loss: 0.000029(61.8), 2nd loss: 0.000598(61.8), 3rd loss: 1.581640(47.3)\n",
      "平均損失: 0.161925, 1st loss: 0.000063(47.6), 2nd loss: 0.000818(47.6), 3rd loss: 1.796657(46.7)\n",
      "平均損失: 0.112869, 1st loss: 0.000023(70.4), 2nd loss: 0.000460(70.1), 3rd loss: 1.252564(58.7)\n",
      "平均損失: 0.059089, 1st loss: 0.000023(74.6), 2nd loss: 0.000202(72.9), 3rd loss: 0.655635(56.4)\n",
      "平均損失: 0.117412, 1st loss: 0.000028(68.7), 2nd loss: 0.000453(68.7), 3rd loss: 1.302942(52.1)\n",
      "平均損失: 0.068441, 1st loss: 0.000023(76.1), 2nd loss: 0.000241(75.8), 3rd loss: 0.759441(58.1)\n",
      "平均損失: 0.062726, 1st loss: 0.000023(76.6), 2nd loss: 0.000203(76.4), 3rd loss: 0.696002(57.3)\n",
      "平均損失: 0.087911, 1st loss: 0.000022(75.8), 2nd loss: 0.000290(75.5), 3rd loss: 0.975565(64.7)\n",
      "平均損失: 0.091304, 1st loss: 0.000023(75.8), 2nd loss: 0.000306(75.8), 3rd loss: 1.013216(62.1)\n",
      "平均損失: 0.086692, 1st loss: 0.000023(76.4), 2nd loss: 0.000279(76.4), 3rd loss: 0.962026(64.4)\n",
      "平均損失: 0.085169, 1st loss: 0.000023(76.1), 2nd loss: 0.000271(76.1), 3rd loss: 0.945127(65.8)\n",
      "平均損失: 0.087404, 1st loss: 0.000023(75.8), 2nd loss: 0.000280(75.8), 3rd loss: 0.969933(62.7)\n",
      "平均損失: 0.087114, 1st loss: 0.000023(75.8), 2nd loss: 0.000280(75.8), 3rd loss: 0.966716(66.1)\n",
      "平均損失: 0.087342, 1st loss: 0.000023(75.5), 2nd loss: 0.000280(75.5), 3rd loss: 0.969245(63.5)\n",
      "平均損失: 0.087233, 1st loss: 0.000023(75.2), 2nd loss: 0.000280(75.2), 3rd loss: 0.968034(63.8)\n",
      "平均損失: 0.087171, 1st loss: 0.000023(75.2), 2nd loss: 0.000276(75.2), 3rd loss: 0.967339(62.4)\n",
      "平均損失: 0.087037, 1st loss: 0.000023(74.9), 2nd loss: 0.000280(74.9), 3rd loss: 0.965855(62.4)\n",
      "平均損失: 0.087320, 1st loss: 0.000023(74.6), 2nd loss: 0.000280(74.6), 3rd loss: 0.968994(61.0)\n",
      "平均損失: 0.087175, 1st loss: 0.000023(74.6), 2nd loss: 0.000280(74.6), 3rd loss: 0.967385(63.8)\n",
      "平均損失: 0.087129, 1st loss: 0.000023(74.6), 2nd loss: 0.000280(74.6), 3rd loss: 0.966874(62.7)\n",
      "平均損失: 0.085285, 1st loss: 0.000023(74.9), 2nd loss: 0.000271(74.9), 3rd loss: 0.946405(64.7)\n",
      "平均損失: 0.085246, 1st loss: 0.000023(74.9), 2nd loss: 0.000271(74.9), 3rd loss: 0.945976(63.5)\n",
      "平均損失: 0.082987, 1st loss: 0.000023(75.2), 2nd loss: 0.000259(75.2), 3rd loss: 0.920903(63.8)\n",
      "平均損失: 0.082972, 1st loss: 0.000023(75.2), 2nd loss: 0.000259(75.2), 3rd loss: 0.920732(64.7)\n",
      "平均損失: 0.080545, 1st loss: 0.000023(75.5), 2nd loss: 0.000247(75.5), 3rd loss: 0.893798(65.2)\n",
      "平均損失: 0.079797, 1st loss: 0.000023(75.8), 2nd loss: 0.000244(75.8), 3rd loss: 0.885491(64.1)\n",
      "平均損失: 0.079792, 1st loss: 0.000023(75.8), 2nd loss: 0.000244(75.8), 3rd loss: 0.885435(65.2)\n",
      "平均損失: 0.080073, 1st loss: 0.000023(75.8), 2nd loss: 0.000244(75.8), 3rd loss: 0.888553(65.5)\n",
      "平均損失: 0.079637, 1st loss: 0.000023(75.8), 2nd loss: 0.000244(75.8), 3rd loss: 0.883718(64.4)\n",
      "平均損失: 0.079686, 1st loss: 0.000023(75.8), 2nd loss: 0.000244(75.8), 3rd loss: 0.884252(64.1)\n",
      "平均損失: 0.080851, 1st loss: 0.000023(76.1), 2nd loss: 0.000240(76.1), 3rd loss: 0.897183(61.8)\n",
      "平均損失: 0.080086, 1st loss: 0.000023(76.1), 2nd loss: 0.000240(76.1), 3rd loss: 0.888692(64.7)\n",
      "平均損失: 0.078971, 1st loss: 0.000023(76.1), 2nd loss: 0.000236(76.1), 3rd loss: 0.876321(64.7)\n",
      "平均損失: 0.079319, 1st loss: 0.000024(76.1), 2nd loss: 0.000236(76.1), 3rd loss: 0.880178(64.1)\n",
      "平均損失: 0.079826, 1st loss: 0.000024(76.1), 2nd loss: 0.000236(76.1), 3rd loss: 0.885806(63.8)\n",
      "平均損失: 0.079207, 1st loss: 0.000024(75.8), 2nd loss: 0.000236(75.8), 3rd loss: 0.878941(60.4)\n",
      "平均損失: 0.078576, 1st loss: 0.000024(75.8), 2nd loss: 0.000236(75.8), 3rd loss: 0.871938(65.8)\n",
      "平均損失: 0.079614, 1st loss: 0.000024(75.8), 2nd loss: 0.000236(75.8), 3rd loss: 0.883459(61.8)\n",
      "平均損失: 0.078789, 1st loss: 0.000024(76.1), 2nd loss: 0.000234(76.1), 3rd loss: 0.874294(63.8)\n",
      "平均損失: 0.079653, 1st loss: 0.000024(76.1), 2nd loss: 0.000234(76.1), 3rd loss: 0.883889(63.8)\n",
      "平均損失: 0.079534, 1st loss: 0.000024(76.1), 2nd loss: 0.000234(76.1), 3rd loss: 0.882563(61.8)\n",
      "=======================================================================================================================\n",
      "No.3 trial:\n",
      "平均損失: 0.165286, 1st loss: 0.000690(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.164790, 1st loss: 0.000139(45.6), 2nd loss: 0.000843(45.6), 3rd loss: 1.827693(45.6)\n",
      "平均損失: 0.165636, 1st loss: 0.000070(46.7), 2nd loss: 0.000837(45.3), 3rd loss: 1.837780(45.3)\n",
      "平均損失: 0.174079, 1st loss: 0.000044(50.7), 2nd loss: 0.000825(45.3), 3rd loss: 1.931756(45.6)\n",
      "平均損失: 0.155514, 1st loss: 0.000040(52.4), 2nd loss: 0.000786(46.4), 3rd loss: 1.725724(46.2)\n",
      "平均損失: 0.124989, 1st loss: 0.000028(63.0), 2nd loss: 0.000561(53.3), 3rd loss: 1.387034(49.9)\n",
      "平均損失: 0.118651, 1st loss: 0.000026(65.8), 2nd loss: 0.000488(56.7), 3rd loss: 1.316716(49.3)\n",
      "平均損失: 0.160249, 1st loss: 0.000053(49.0), 2nd loss: 0.000806(47.6), 3rd loss: 1.778154(46.7)\n",
      "平均損失: 0.082115, 1st loss: 0.000023(69.5), 2nd loss: 0.000347(68.1), 3rd loss: 0.911207(52.7)\n",
      "平均損失: 0.139567, 1st loss: 0.000037(61.0), 2nd loss: 0.000618(61.0), 3rd loss: 1.548760(49.9)\n",
      "平均損失: 0.084485, 1st loss: 0.000023(72.1), 2nd loss: 0.000335(70.9), 3rd loss: 0.937520(56.1)\n",
      "平均損失: 0.095160, 1st loss: 0.000024(69.5), 2nd loss: 0.000392(68.7), 3rd loss: 1.055994(54.1)\n",
      "平均損失: 0.108080, 1st loss: 0.000026(68.9), 2nd loss: 0.000463(68.7), 3rd loss: 1.199386(61.3)\n",
      "平均損失: 0.101188, 1st loss: 0.000023(69.2), 2nd loss: 0.000427(68.9), 3rd loss: 1.122914(53.8)\n",
      "平均損失: 0.091897, 1st loss: 0.000023(70.1), 2nd loss: 0.000379(69.8), 3rd loss: 1.019799(60.7)\n",
      "平均損失: 0.092353, 1st loss: 0.000023(69.5), 2nd loss: 0.000392(69.2), 3rd loss: 1.024854(56.7)\n",
      "平均損失: 0.096110, 1st loss: 0.000023(70.4), 2nd loss: 0.000400(70.4), 3rd loss: 1.066549(55.0)\n",
      "平均損失: 0.096541, 1st loss: 0.000023(70.1), 2nd loss: 0.000403(70.1), 3rd loss: 1.071334(58.1)\n",
      "平均損失: 0.089759, 1st loss: 0.000023(70.4), 2nd loss: 0.000389(70.1), 3rd loss: 0.996057(61.5)\n",
      "平均損失: 0.089729, 1st loss: 0.000023(70.4), 2nd loss: 0.000384(70.4), 3rd loss: 0.995722(61.5)\n",
      "平均損失: 0.091320, 1st loss: 0.000023(70.7), 2nd loss: 0.000387(70.7), 3rd loss: 1.013379(62.4)\n",
      "平均損失: 0.088456, 1st loss: 0.000023(71.2), 2nd loss: 0.000380(71.2), 3rd loss: 0.981595(63.2)\n",
      "平均損失: 0.087706, 1st loss: 0.000023(71.2), 2nd loss: 0.000379(71.2), 3rd loss: 0.973265(63.8)\n",
      "平均損失: 0.086806, 1st loss: 0.000023(71.2), 2nd loss: 0.000376(71.2), 3rd loss: 0.963282(63.2)\n",
      "平均損失: 0.086702, 1st loss: 0.000023(71.5), 2nd loss: 0.000374(71.5), 3rd loss: 0.962125(61.5)\n",
      "平均損失: 0.086651, 1st loss: 0.000023(71.8), 2nd loss: 0.000369(71.8), 3rd loss: 0.961564(62.7)\n",
      "平均損失: 0.087028, 1st loss: 0.000023(71.5), 2nd loss: 0.000369(71.5), 3rd loss: 0.965743(60.1)\n",
      "平均損失: 0.085674, 1st loss: 0.000023(71.8), 2nd loss: 0.000361(71.8), 3rd loss: 0.950714(61.3)\n",
      "平均損失: 0.084945, 1st loss: 0.000023(72.1), 2nd loss: 0.000357(72.1), 3rd loss: 0.942619(61.3)\n",
      "平均損失: 0.084744, 1st loss: 0.000023(72.1), 2nd loss: 0.000357(72.1), 3rd loss: 0.940388(63.0)\n",
      "平均損失: 0.084859, 1st loss: 0.000023(72.1), 2nd loss: 0.000357(72.1), 3rd loss: 0.941671(61.5)\n",
      "平均損失: 0.085494, 1st loss: 0.000023(72.1), 2nd loss: 0.000357(71.8), 3rd loss: 0.948714(58.4)\n",
      "平均損失: 0.084807, 1st loss: 0.000023(72.1), 2nd loss: 0.000357(72.1), 3rd loss: 0.941092(61.5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m search_space \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim/lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.006\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/kernal/ks1_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m21\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining/ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 27\u001b[0m \u001b[43moptimal_perform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m, in \u001b[0;36moptimal_perform\u001b[0;34m(search_space)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======================================================================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model, train_loss, test_loss, test_acc_1, test_acc_2, test_acc_3, output1_record_train, output2_record_train, output3_record_train, output1_record_test, output2_record_test, output3_record_test \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mray_tune\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, reporter, x_train_id, y_train_id, x_test_id, y_test_id, ray_tune, model_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m output3_record_train\u001b[38;5;241m.\u001b[39mappend(output3_record)\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 55\u001b[0m model, test_loss_iter, test_accuracy_iter, output1_record, output2_record, output3_record \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mray_tune\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mray_tune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m test_loss\u001b[38;5;241m.\u001b[39mappend(test_loss_iter)\n\u001b[1;32m     57\u001b[0m test_acc_1\u001b[38;5;241m.\u001b[39mappend(test_accuracy_iter[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(epoch, model, criterion, test_loader, ray_tune)\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     17\u001b[0m output, x_scale_1, x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x_cropped, x_scale_2, x_scale_3, x3_1, x3_2, x_cropped2 \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 19\u001b[0m loss, loss_1, loss_2, loss_3 \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m test_loss_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     21\u001b[0m test_loss1_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mcustomLoss.forward\u001b[0;34m(self, output, target)\u001b[0m\n\u001b[1;32m     23\u001b[0m MSE \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m loss_1 \u001b[38;5;241m=\u001b[39m BCE(output[\u001b[38;5;241m0\u001b[39m], target[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m loss_2 \u001b[38;5;241m=\u001b[39m MSE(torch\u001b[38;5;241m.\u001b[39msum(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39msum(target[\u001b[38;5;241m1\u001b[39m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     27\u001b[0m loss_3 \u001b[38;5;241m=\u001b[39m MSE(torch\u001b[38;5;241m.\u001b[39msum(output[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m target[\u001b[38;5;241m0\u001b[39m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39msum(target[\u001b[38;5;241m2\u001b[39m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     29\u001b[0m loss_total \u001b[38;5;241m=\u001b[39m weight[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_1 \u001b[38;5;241m+\u001b[39m weight[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_2 \u001b[38;5;241m+\u001b[39m weight[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_3\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"optim/lr\": 0.006,\n",
    "    \"model/kernal/ks1_1\": 21,\n",
    "    \"model/kernal/ks1_4\": 3,\n",
    "    \"model/kernal/ks3_1\": 21,\n",
    "    \"model/kernal/ks3_2\": 5,\n",
    "    \"model/kernal/ks4_1\": 3,\n",
    "    \"model/kernal/ks4_2\": 3,\n",
    "    \"training/batch\": 256,\n",
    "    \"model/kernal/ratio\": 1.9,\n",
    "    \"model/width/out_channel7\": 100,\n",
    "    \"model/pooling\": 3,\n",
    "    \"loss/l1\": 100,\n",
    "    \"loss/l2\": 1,\n",
    "    \"loss/l3\": 10,\n",
    "    \"model/width/w1\": 256,\n",
    "    \"model/width/w2\": 256,\n",
    "    \"model/width/w3\": 256,\n",
    "    \"data/re1\": \"mean\",\n",
    "    \"data/re2\": \"mean\",\n",
    "    \"data/re3\": \"mean\",\n",
    "    \"loss/LAE_w\": 1,\n",
    "    \"training/ratio\": 1\n",
    "}\n",
    "\n",
    "\n",
    "optimal_perform(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zaNyGbs-JG2",
    "outputId": "5dbdd51d-fdd8-4b6a-fe1c-917eb2151600",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** SIGTERM received at time=1714680291 on cpu 62 ***\n",
      "PC: @     0x7fbf8b2a888f  (unknown)  epoll_wait\n",
      "    @     0x7fbf8b4f4910  (unknown)  (unknown)\n",
      "    @           0x73e840  (unknown)  (unknown)\n",
      "[2024-05-02 13:04:51,631 E 722993 722993] logging.cc:361: *** SIGTERM received at time=1714680291 on cpu 62 ***\n",
      "[2024-05-02 13:04:51,631 E 722993 722993] logging.cc:361: PC: @     0x7fbf8b2a888f  (unknown)  epoll_wait\n",
      "[2024-05-02 13:04:51,635 E 722993 722993] logging.cc:361:     @     0x7fbf8b4f4910  (unknown)  (unknown)\n",
      "[2024-05-02 13:04:51,640 E 722993 722993] logging.cc:361:     @           0x73e840  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"optim/lr\": tune.grid_search([0.006]),\n",
    "    \"model/kernal/ks1_1\": tune.grid_search([3]),\n",
    "    \"model/kernal/ks1_4\": tune.grid_search([3]),\n",
    "    \"model/kernal/ks3_1\": tune.grid_search([3]),\n",
    "    \"model/kernal/ks3_2\": tune.grid_search([3]),\n",
    "    \"model/kernal/ks4_1\": tune.grid_search([3]),\n",
    "    \"model/kernal/ks4_2\": tune.grid_search([3]),\n",
    "    \"training/batch\": tune.grid_search([64]),\n",
    "    \"model/kernal/ratio\": tune.grid_search([1]),\n",
    "    \"model/width/out_channel7\": tune.grid_search([1024]),\n",
    "    \"model/pooling\": tune.grid_search([3]),\n",
    "    \"loss/l1\": tune.grid_search([1]),\n",
    "    \"loss/l2\": tune.grid_search([0]),\n",
    "    \"loss/l3\": tune.grid_search([0]),\n",
    "    \"model/width/w1\": tune.grid_search([512]),\n",
    "    \"model/width/w2\": tune.grid_search([512]),\n",
    "    \"model/width/w3\": tune.grid_search([512]),\n",
    "    \"data/re1\": tune.grid_search([\"mean\"]),\n",
    "    \"data/re2\": tune.grid_search([\"mean\"]),\n",
    "    \"data/re3\": tune.grid_search([\"mean\"]),\n",
    "    \"loss/LAE_w\": tune.grid_search([5]),\n",
    "    \"training/ratio\": tune.grid_search([1])\n",
    "    # \"momentum\": tune.grid_search([1.1]),\n",
    "    # \"weight_decay\": tune.grid_search([0])\n",
    "}\n",
    "\n",
    "trainable_with_gpu = tune.with_resources(build_model, {\"gpu\": 1})\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_gpu,\n",
    "    param_space = search_space,\n",
    "    tune_config = tune.TuneConfig(num_samples = 2)\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ~/ray_results\n",
    "nersc_tensorboard_helper.tb_address()\n",
    "\n",
    "df = results.get_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "R_H0efrzpNuz",
    "outputId": "ab40c624-6135-4280-ae39-c7d4fd731163",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(f\"../DESI_LAE_model/model_0.979_0.979_0.881_2024-02-07 07:13:17.397448.pt\")\n",
    "output, refer, diff = test_metrics(model, plot_wrong = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "08Sor68hhBGX",
    "outputId": "435affd2-87f2-4f0c-8246-a14dc0086e46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss_iter(test_loss = [i.item() for i in test_loss], train_loss = [i.item() for i in train_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "4hLwGjgTiZ7g",
    "outputId": "9e407f5b-8150-4686-d572-0bb262066fd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_test_accuracy(test_acc1 = test_acc_1, test_acc2 = test_acc_2, test_acc3 = test_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L7xhea6rY-Ls",
    "outputId": "9032f088-28b2-4ac9-ddea-fced063667c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(output1_record_train) - 1):\n",
    "  plt.close()\n",
    "  plt.figure(figsize = (16, 5))\n",
    "  plot_scatter(x = output1_record_train[i][1].to(\"cpu\"), y = output1_record_train[i][0].to(\"cpu\"), c = \"m\", ylabel = \"Predicted Position\", xlabel = \"True Position\", position = 1)\n",
    "  plot_scatter(x = output2_record_train[i][1].to(\"cpu\"), y = output2_record_train[i][0].to(\"cpu\"), c = \"m\", ylabel = \"Predicted Percentage\", xlabel = \"True Percentage\", position = 2)\n",
    "  plot_scatter(x = output3_record_train[i][1].to(\"cpu\"), y = output3_record_train[i][0].to(\"cpu\"), c = \"m\", ylabel = \"Predicted FWHM\", xlabel = \"True FWHM\", position = 3)\n",
    "\n",
    "  plot_scatter(x = output1_record_test[i][1].to(\"cpu\"), y = output1_record_test[i][0].to(\"cpu\"), c = \"y\", ylabel = \"Predicted Position\", xlabel = \"True Position\", position = 1)\n",
    "  plot_scatter(x = output2_record_test[i][1].to(\"cpu\"), y = output2_record_test[i][0].to(\"cpu\"), c = \"y\", ylabel = \"Predicted Percentage\", xlabel = \"True Percentage\", position = 2)\n",
    "  plot_scatter(x = output3_record_test[i][1].to(\"cpu\"), y = output3_record_test[i][0].to(\"cpu\"), c = \"y\", ylabel = \"Predicted FWHM\", xlabel = \"True FWHM\", position = 3)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcgnNE-3dQE9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_cropped2 = check_LAE(model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bSvkDC16noR8",
    "outputId": "570a0dbf-66d5-4c57-b04a-5bde6a585e5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in x_cropped2:\n",
    "  plt.close()\n",
    "  plt.plot(i[0].to(\"cpu\"))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia5ZyM9IkAOs"
   },
   "outputs": [],
   "source": [
    "spectra_train, spectra_test = train_test_split(LAE_spectra, test_size = 0.2, random_state = 2)\n",
    "x_range = np.linspace(0, 2438, x_cropped2.shape[-1])\n",
    "for i in range(0, x_cropped2.shape[0]):\n",
    "  plt.close()\n",
    "  plt.rcParams['figure.figsize'] = [16, 4]\n",
    "  plt.plot(x_range, x_scale_1[i][0].to(\"cpu\"), c = \"k\", lw = 1)\n",
    "  plt.plot(spectra_test[i][\"flux\"], alpha = 0.5, c = \"r\", lw = 1)\n",
    "  plt.title(spectra_test[i]['specid'])\n",
    "  plt.savefig(\"/content/drive/MyDrive/CNN/result/x_scale_1/\" + str(spectra_test[i]['specid']) + \".jpg\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGe7N8eZqjzU"
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, len(x_cropped[0][0]), x_scale_3.shape[-1])\n",
    "for i in range(0, x_scale_3.shape[0]):\n",
    "  plt.close()\n",
    "  plt.rcParams['figure.figsize'] = [16, 4]\n",
    "  # plt.plot(x_range, x_cropped[i][0].to(\"cpu\"), c = \"k\", lw = 1, alpha = 0.5)\n",
    "  # plt.plot(x_range, x_cropped[i][1].to(\"cpu\"), c = \"r\", lw = 1, alpha = 0.5)\n",
    "  plt.plot(x_range, x_scale_2[i][0].to(\"cpu\"), c = \"k\", lw = 1)\n",
    "  plt.plot(x_range, x_scale_2[i][1].to(\"cpu\"), c = \"r\", lw = 1)\n",
    "  plt.title(spectra_test[i]['specid'])\n",
    "  plt.savefig(\"/content/drive/MyDrive/CNN/result/x_scale_3/\" + str(spectra_test[i]['specid']) + \".jpg\")\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
